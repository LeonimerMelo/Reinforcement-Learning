{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLvLye0bqa5X3mqay6qzoP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonimerMelo/Reinforcement-Learning/blob/Q-Learning/Markov_Decision_Process_(MDP)_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Markov Decision Process (MDP)\n",
        "Markov Decision Process (MDP) é uma ferramenta matemática estocástica (determinada aleatoriamente) baseada no conceito de propriedade de Markov. É usado para modelar problemas de tomada de decisão onde os resultados são parcialmente aleatórios e parcialmente controláveis, e para ajudar a tomar decisões ótimas dentro de um sistema dinâmico. A propriedade de Markov expressa que em um processo aleatório, a probabilidade de um estado futuro ocorrer depende apenas do estado atual, e não depende de nenhum estado passado ou futuro. É um *framework* que pode ajudar a resolver a maioria dos problemas de aprendizagem por reforço (RL).\n",
        "\n",
        "A **Markov process** or **Markov chain** is a stochastic process of states that is memoryless, or where the probability of any future state $(S_t+1)$ occurring is only dependent on its current state $(S_t)$, and is independent of any past or future states."
      ],
      "metadata": {
        "id": "4A_OdFgbSzvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markov Decision Process (MDP) é um modelo matemático utilizado para representar problemas de decisão sequencial em ambientes incertos. É amplamente utilizado em:\n",
        "\n",
        "Aplicações\n",
        "\n",
        "1. Inteligência Artificial (IA)\n",
        "2. Aprendizado de Máquina (ML)\n",
        "3. Robótica\n",
        "4. Automação\n",
        "5. Economia\n",
        "6. Finanças\n",
        "7. Logística\n",
        "8. Planejamento de Produção\n",
        "\n",
        "Componentes\n",
        "\n",
        "1. Estados (S): Conjunto de estados possíveis.\n",
        "2. Ações (A): Conjunto de ações possíveis.\n",
        "3. Probabilidade de Transição (P): Probabilidade de transição entre estados.\n",
        "4. Função de Recompensa (R): Recompensa associada a cada ação em cada estado.\n",
        "5. Política (π): Estratégia de tomada de decisão.\n",
        "\n",
        "Características\n",
        "\n",
        "1. **Markoviano**: *O futuro depende apenas do presente*.\n",
        "2. Decisão sequencial.\n",
        "3. Ambiente incerto.\n",
        "4. Recompensa imediata e/ou futura.\n",
        "\n",
        "Tipos de MDP\n",
        "\n",
        "1. MDP Finito: Número finito de estados e ações.\n",
        "2. MDP Contínuo: Estados e/ou ações contínuos.\n",
        "3. MDP Parcialmente Observável (PO-MDP): Estados não totalmente observáveis.\n",
        "\n",
        "Algoritmos\n",
        "\n",
        "1. Value Iteration (VI)\n",
        "2. Policy Iteration (PI)\n",
        "3. Q-Learning\n",
        "4. SARSA\n",
        "5. Deep Q-Networks (DQN)\n",
        "\n",
        "Ferramentas\n",
        "\n",
        "1. Gymnasium [Gym] (Python)\n",
        "2. PyMDP (Python)\n",
        "3. MDPtoolbox (Matlab)\n",
        "4. CVXPY (Python)"
      ],
      "metadata": {
        "id": "CvF5mXRFSsD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markov Decision Process (MDP)** é uma estrutura matemática usada para descrever a tomada de decisões em situações onde os resultados são parcialmente aleatórios e parcialmente sob o controle de um agente. Ela fornece um modelo formal para problemas de decisão sequenciais e é amplamente utilizada em áreas como aprendizado por reforço, pesquisa operacional e economia.\n",
        "\n",
        "Um MDP é definido pelos seguintes componentes:\n",
        "\n",
        "1. **Estados (S)**: Um conjunto de estados possíveis que descrevem todas as configurações ou situações possíveis em que o sistema pode se encontrar.\n",
        "   \n",
        "2. **Ações (A)**: Um conjunto de ações que o agente pode escolher. A escolha que o agente faz no *step time* atual. A escolha de ação determina como o sistema transita entre os estados. Por exemplo, o robô pode mover sua perna direita ou esquerda, levantar seu braço, levantar um objeto ou virar para a direita/esquerda, etc. O conjunto de ações (decisões) que o agente pode executar é conhecido com antecedência.\n",
        "\n",
        "3. **Função de Transição (P)**: Uma distribuição de probabilidade que especifica a probabilidade de transitar de um estado para outro dado uma ação específica. Formalmente, é $( P(s'|s, a) )$, onde:\n",
        "   - $( s )$ é o estado atual.\n",
        "   - $( a )$ é a ação tomada.\n",
        "   - $( s' )$ é o próximo estado.\n",
        "\n",
        "4. **Função de Recompensa (R)**: Uma função que atribui a recompensa imediata recebida após transitar de um estado para outro devido a uma ação particular. Isso é geralmente denotado como $( R(s, a, s') )$, onde:\n",
        "   - $( s )$ é o estado atual.\n",
        "   - $( a )$ é a ação.\n",
        "   - $( s' )$ é o estado resultante após a ação ser tomada.\n",
        "\n",
        "5. **Fator de Desconto ($γ$)**: É o fator $( 0 \\leq \\gamma \\leq 1 )$ que determina a importância das recompensas futuras em comparação com as recompensas imediatas. Um fator de desconto mais próximo de 0 enfatiza recompensas de curto prazo, enquanto um fator mais próximo de 1 prioriza recompensas de longo prazo.\n",
        "\n",
        "6. **Política (π)**: Uma estratégia ou regra que o agente segue para decidir qual ação tomar em cada estado. Uma política pode ser determinística ou probabilística. Uma política é o processo de raciocínio por trás da escolha de uma ação. Na prática, é uma distribuição de probabilidade atribuída ao conjunto de ações. Ações altamente recompensadoras terão uma alta probabilidade e vice-versa. Se uma ação tem uma baixa probabilidade, isso não significa que ela não será escolhida. É apenas menos provável que seja escolhida.\n",
        "\n",
        "O objetivo do agente em um MDP é tipicamente encontrar uma **política ótima** que maximize a recompensa acumulada ao longo do tempo, o que é frequentemente chamado de **retorno**. No caso de recompensas descontadas, o retorno é calculado da seguinte forma:\n",
        "\n",
        "$$\n",
        "R_t = \\sum_{k=0}^{\\infty} \\gamma^k R(s_t, a_t, s_{t+1})\n",
        "$$\n",
        "\n",
        "Onde $ R_t $ é o retorno total a partir do passo de tempo $ t $, e $ \\gamma^k $ é o fator de desconto elevado à potência do passo de tempo.\n",
        "\n",
        "### Conceitos Principais:\n",
        "- **Função de Valor-Estado (V(s))**: O retorno esperado a partir do estado $ s $ seguindo uma política $ \\pi $ dada. Representa o quão bom é estar em um determinado estado.\n",
        "  \n",
        "- **Função de Valor-Ação (Q(s, a))**: O retorno esperado a partir do estado $ s $, tomando a ação $ a $, e depois seguindo uma política $ \\pi $. É usada para avaliar qual ação é a melhor a ser tomada em um dado estado.\n",
        "\n",
        "### Resolvendo um MDP:\n",
        "Para resolver um MDP, algoritmos como **Iteração de Valor** e **Iteração de Política** são frequentemente usados. Esses métodos melhoram iterativamente a função de valor e a política para encontrar a estratégia de tomada de decisão ótima para o agente.\n",
        "\n",
        "Os MDPs fornecem uma estrutura poderosa para modelar e resolver problemas de tomada de decisão sob incerteza e são fundamentais para o aprendizado por reforço."
      ],
      "metadata": {
        "id": "FuNLfCgZRr7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Markov Property\n",
        "Imagine que um robô sentado em uma cadeira se levantou e colocou seu pé direito para frente. Então, no momento, ele está de pé com seu pé direito para frente. Este é seu estado atual.\n",
        "\n",
        "Agora, de acordo com a propriedade de Markov, o estado atual do robô depende apenas de seu estado anterior imediato (ou do *time step* anterior), ou seja, o estado em que ele estava quando se levantou. Não depende de seus estados anteriores — antes de estar sentado na cadeira. Da mesma forma, seu próximo estado depende apenas de seu estado atual.\n",
        "\n",
        "Formalmente, para que um estado $ S_t $ seja Markoviano, a probabilidade do próximo estado $ S_{t+1} $ ser $ s' $ deve depender apenas do estado atual $ S_t = s_t $, e não dos estados passados $ S_{t-1}, S_{t-2}, \\dots $.\n",
        "\n",
        "Um estado $ S_t $ é **Markov** se, e somente se, satisfizer a **propriedade de Markov**. Essa propriedade pode ser expressa matematicamente como:\n",
        "\n",
        "$$\n",
        "P(S_{t+1} | S_t, S_{t-1}, \\dots, S_0, A_t, A_{t-1}, \\dots, A_0) = P(S_{t+1} | S_t, A_t),\n",
        "$$\n",
        "\n",
        "onde:\n",
        "- $ S_t $ representa o estado no tempo $ t $,\n",
        "- $ A_t $ representa a ação tomada no tempo $ t $,\n",
        "- $ P(S_{t+1} | \\cdot) $ representa a probabilidade condicional de transição para o próximo estado $ S_{t+1} $.\n",
        "\n",
        "### Interpretação:\n",
        "Essa propriedade significa que o futuro estado $ S_{t+1} $ depende apenas do estado atual $ S_t $ e da ação atual $ A_t $, e não do histórico de estados e ações passados. Em outras palavras, **o estado atual encapsula todas as informações necessárias para prever o futuro**.\n",
        "\n",
        "### Implicações Práticas:\n",
        "- **Dinâmica sem memória**: O processo não \"lembra\" do passado; as dinâmicas do sistema não dependem de estados/ações anteriores, uma vez que o estado atual é conhecido.\n",
        "- **Processos de Decisão de Markov (MDPs)**: Essa propriedade é fundamental para definir os MDPs, que são a base matemática de muitos algoritmos de Aprendizado por Reforço.\n",
        "\n",
        "Se o estado $ S_t $ não satisfaz a propriedade de Markov, informações adicionais (como estados ou ações passadas) podem precisar ser incluídas na representação do estado para torná-lo Markoviano."
      ],
      "metadata": {
        "id": "-oudNSlZcM8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Processos de Decisão de Markov (MDPs)**\n",
        "\n",
        "Um **Processo de Decisão de Markov** (Markov Decision Process - MDP) é uma estrutura matemática utilizada para modelar problemas de decisão sequencial em ambientes onde o resultado depende tanto das ações do agente quanto das dinâmicas estocásticas do ambiente. O MDP assume que o estado atual contém toda a informação necessária para determinar o comportamento futuro (propriedade de Markov).\n",
        "\n",
        "---\n",
        "\n",
        "### **Definição Formal**\n",
        "Um MDP é definido como um **tuplo $ (S, A, P, R, \\gamma) $**, onde:\n",
        "\n",
        "1. **$ S $: Conjunto de Estados**\n",
        "   - Representa todos os possíveis estados $ s $ do ambiente.\n",
        "   - Exemplo: A posição de um robô em uma sala.\n",
        "\n",
        "2. **$ A $: Conjunto de Ações**\n",
        "   - Representa todas as ações $ a $ disponíveis ao agente em cada estado.\n",
        "   - Exemplo: Mover para cima, para baixo, para a esquerda ou para a direita.\n",
        "\n",
        "3. **$ P(s' | s, a) $: Probabilidade de Transição**\n",
        "   - Representa a probabilidade de o estado $ s' $ ser alcançado ao tomar a ação $ a $ no estado $ s $.\n",
        "   - Exemplo: Em um ambiente estocástico, mover \"para cima\" pode resultar em 80% de chance de subir e 20% de chance de deslizar para o lado.\n",
        "\n",
        "4. **$ R(s, a) $: Função de Recompensa**\n",
        "   - Representa a recompensa esperada por executar a ação $ a $ no estado $ s $.\n",
        "   - Exemplo: Receber uma recompensa de +10 ao alcançar um objetivo ou -1 por cada movimento realizado.\n",
        "\n",
        "5. **$ \\gamma $: Fator de Desconto**\n",
        "   - Um valor $ \\gamma \\in [0, 1] $ que determina a importância das recompensas futuras.\n",
        "   - Exemplo: $ \\gamma = 0.9 $ significa que recompensas futuras são reduzidas em 10% a cada passo no tempo.\n",
        "\n",
        "---\n",
        "\n",
        "### Objetivo no MDP\n",
        "O objetivo é encontrar uma **política ótima ($\\pi^*$)**, que é uma função mapeando estados para ações ($\\pi(s)$), de forma que maximize a recompensa acumulada esperada ($G_t$):\n",
        "\n",
        "$$\n",
        "G_t = \\mathbb{E}\\left[\\sum_{k=0}^\\infty \\gamma^k R(s_{t+k}, a_{t+k}, s_{t+k+1})\\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Soluções para MDPs**\n",
        "Existem métodos clássicos para resolver MDPs, como:\n",
        "\n",
        "1. **Iteração de Valor**:\n",
        "   - Atualiza iterativamente a função de valor dos estados $ V(s) $ até a convergência.\n",
        "   - Baseia-se na Equação de Bellman:\n",
        "     $$\n",
        "     V(s) = \\max_a \\sum_{s'} P(s' | s, a) \\left[ R(s, a) + \\gamma V(s') \\right].\n",
        "     $$\n",
        "\n",
        "2. **Iteração de Política**:\n",
        "   - Alterna entre avaliar uma política atual $ \\pi $ e melhorá-la para $ \\pi' $.\n",
        "\n",
        "3. **Métodos de Programação Dinâmica**:\n",
        "   - Utilizam a estrutura do MDP e a Equação de Bellman para encontrar soluções de forma eficiente.\n",
        "\n",
        "4. **Algoritmos de Aprendizado por Reforço**:\n",
        "   - Quando o modelo $ P(s'|s,a) $ e $ R(s,a) $ não são conhecidos, métodos como Q-Learning ou SARSA podem ser usados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Aplicações de MDPs**\n",
        "MDPs são amplamente utilizados em diversas áreas, incluindo:\n",
        "- **Robótica**: Planejamento de movimentos.\n",
        "- **Jogos**: Desenvolvimento de IA para jogos como xadrez e videogames.\n",
        "- **Negócios**: Otimização de estoques e gerenciamento de recursos.\n",
        "- **Saúde**: Planejamento de tratamentos médicos.\n",
        "- **Sistemas Autônomos**: Veículos autônomos e drones."
      ],
      "metadata": {
        "id": "rSe2Vo13rFW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objetivo de um Markov Decision Process (MDP)\n",
        "O **objetivo em um Markov Decision Process (MDP)** é encontrar uma **política ótima ($\\pi^*$)** que maximize a recompensa acumulada esperada ao longo do tempo. Vamos detalhar o que isso significa e como é calculado:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. O que é uma política ($\\pi$)?**\n",
        "- Uma política é uma regra que define qual ação tomar em cada estado.\n",
        "- Exemplo: Para um rato em um labirinto, a política pode ser algo como: \"Se estou na célula $s$, me movo para a direita.\"\n",
        "\n",
        "A política pode ser:\n",
        "- **Determinística ($\\pi(s) = a$)**: Sempre escolhe a mesma ação para um dado estado.\n",
        "- **Estocástica ($\\pi(a|s)$)**: Escolhe ações com base em probabilidades.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Recompensa acumulada esperada ($G_t$)**\n",
        "O objetivo do agente é maximizar a soma das recompensas ao longo do tempo. Essa soma é chamada de recompensa acumulada esperada ($G_t$) e é calculada como:\n",
        "\n",
        "$$\n",
        "G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k}\n",
        "$$\n",
        "\n",
        "#### **Componentes:**\n",
        "- **$R_t$**: Recompensa recebida no tempo $t$.\n",
        "- **$\\gamma \\in [0, 1]$**: Fator de desconto. Ele ajusta a importância das recompensas futuras:\n",
        "  - Se $\\gamma = 0$, apenas a recompensa imediata importa.\n",
        "  - Se $\\gamma \\to 1$, recompensas futuras têm quase o mesmo peso que as imediatas.\n",
        "- **Soma infinita**: O fator de desconto garante que a soma converge (se $\\gamma < 1$).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Valor de um estado ($V^\\pi(s)$)**\n",
        "O valor de um estado sob uma política $\\pi$ é a recompensa acumulada esperada que o agente receberá, começando no estado $s$ e seguindo a política $\\pi$:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R(s_k, a_k, s_{k+1}) \\mid s_0 = s \\right]\n",
        "$$\n",
        "\n",
        "#### **Como isso funciona:**\n",
        "- O agente começa em $s_0 = s$.\n",
        "- Ele escolhe ações com base na política $\\pi$.\n",
        "- A recompensa total depende das transições e recompensas em cada estado.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. A política ótima ($\\pi^*$)**\n",
        "A política ótima maximiza o valor esperado para todos os estados. Em outras palavras:\n",
        "\n",
        "$$\n",
        "\\pi^* = \\arg\\max_\\pi V^\\pi(s), \\, \\forall s \\in S\n",
        "$$\n",
        "\n",
        "#### **Função de valor ótima ($V^*(s)$):**\n",
        "O valor ótimo de um estado é o maior valor esperado que pode ser obtido de $s$, assumindo que o agente age de forma ótima:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_\\pi V^\\pi(s)\n",
        "$$\n",
        "\n",
        "#### **Função Q-valor ótima ($Q^*(s, a)$):**\n",
        "O valor Q ótimo combina o estado e a ação, indicando a recompensa acumulada esperada ao tomar a ação $a$ no estado $s$ e seguir a política ótima:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E} \\left[ R(s, a, s') + \\gamma V^*(s') \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Resumo do objetivo**\n",
        "O agente deve:\n",
        "1. Avaliar o ambiente para calcular $V^\\pi(s)$ ou $Q^\\pi(s, a)$ para uma política.\n",
        "2. Encontrar a política $\\pi^*$ que maximiza $V^\\pi(s)$ ou $Q^\\pi(s, a)$.\n",
        "3. Seguir essa política para maximizar a soma de recompensas esperadas ao longo do tempo."
      ],
      "metadata": {
        "id": "vXyETqVOFjgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reinforcement Learning as MDP\n",
        "Agora, considere como um ambiente faz a transição de um estado para o próximo usando o que é\n",
        "conhecido como função de transição. No aprendizado por reforço, uma função de transição é\n",
        "formulada como um Markov Decision Process (MDP), que é uma estrutura matemática que\n",
        "modela a tomada de decisão sequencial.\n",
        "\n",
        "Para entender por que as funções de transição são representadas como MDPs, considere uma formulação geral mostrada na equação abaixo:\n",
        "\n",
        "$$s_{t+1}\\simeq \\mathrm{P}\\left[ s_{t+1} | (s_0,a_0), (s_1,a_1),\\cdots ,(s_t,a_t) \\right]$$\n",
        "\n",
        "A equação diz que no *time step* $t$, o próximo estado $s_{t+1}$ é amostrado de uma distribuição de probabilidade\n",
        "$\\mathrm{P}$ *condicionada a todo o histórico*. A probabilidade de um ambiente\n",
        "transitar do estado $s_t$ para $s_{t+1}$ depende de todos os estados precedentes $s$ e ações $a$\n",
        "que ocorreram até agora em um episódio. É desafiador modelar uma função de transição\n",
        "nessa forma, particularmente se os episódios duram muitos passos de tempo. Qualquer função de transição que\n",
        "projetarmos precisaria ser capaz de levar em conta uma vasta combinação de efeitos que ocorreram em\n",
        "qualquer ponto no passado. Além disso, essa formulação torna a função de produção de ação\n",
        "de um agente — sua política — significativamente mais complexa. Como todo o histórico de estados e\n",
        "ações é relevante para entender como uma ação pode mudar o estado futuro do\n",
        "mundo, um agente precisaria levar em conta todas essas informações ao decidir\n",
        "como agir.\n",
        "\n",
        "Para tornar a função de transição de ambiente mais prática, nós a transformamos em um MDP\n",
        "adicionando a suposição de que a transição para o próximo estado $s_{t+1}$ depende apenas\n",
        "do estado anterior $s_{t}$ e da ação $a_t$. Isso é conhecido como propriedade de Markov (**Markov Property**). Com essa\n",
        "suposição, a nova função de transição se torna a seguinte:\n",
        "\n",
        "$$s_{t+1}\\simeq \\mathrm{P}\\left[ s_{t+1} | s_t,a_t \\right]$$\n",
        "\n",
        "A equação diz que o próximo estado $s_{t+1}$ é amostrado de uma distribuição de probabilidade\n",
        "$\\mathrm{P}\\left[ s_{t+1} | s_t,a_t \\right]$. Esta é uma forma mais simples da função de transição original. A propriedade\n",
        "de Markov implica que o estado atual e a ação no *step time t* contêm informações\n",
        "suficientes para determinar completamente a probabilidade de transição para o próximo estado em *t* + 1.\n",
        "\n",
        "Apesar da simplicidade desta formulação, ela ainda é bastante poderosa. Muitos processos\n",
        "podem ser expressos nesta forma, incluindo jogos, controle robótico e planejamento. Isso ocorre porque um estado pode ser definido para incluir todas as informações necessárias para tornar a função de transição Markoviana.\n",
        "\n"
      ],
      "metadata": {
        "id": "d_OEtTw2nXyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora estamos em posição de apresentar a formulação MDP de um problema de aprendizado por reforço.\n",
        "Um MDP é definido por uma tuple[4]: $\\mathcal{S,A,P(.), R(.)}$, onde\n",
        "- $\\mathcal{S}$ é o conjunto de estados.\n",
        "- $\\mathcal{A}$ é o conjunto de ações.\n",
        "- $\\mathcal{P}(s_{t+1} | s_t, a_t)$ é a função de transição de estado do ambiente.\n",
        "- $\\mathcal{R}(s_t, a_t, s_{t+1})$ é a função de recompensa do ambiente.\n",
        "\n",
        "Uma suposição importante subjacente aos problemas de aprendizado por reforço é que os agentes não têm acesso à função de transição $\\mathcal{P}(s_{t+1} | s_t, a_t)$, ou\n",
        "à função de recompensa $\\mathcal{R}(s_t, a_t, s_{t+1})$. A única maneira de um agente obter informações sobre\n",
        "essas funções é por meio dos estados, ações e recompensas que ele realmente experimenta no\n",
        "ambiente — ou seja, as tuplas $(s_t, a_t, r_t)$.\n",
        "\n",
        "Para completar a formulação do problema, também precisamos formalizar o conceito de\n",
        "um objetivo que um agente maximiza. Primeiro, vamos definir o **retorno** $R(τ)$ usando uma\n",
        "trajetória de um episódio, $τ = (s_0, a_0, r_0), \\dots ,(s_T , a_T , r_T )$:\n",
        "\n",
        "$$R(\\tau)=r_0+\\gamma r_1+\\gamma^2 r_2+\\cdots+\\gamma^T r_T=\\sum_{t=0}^{T}\\gamma^t r_t$$\n",
        "\n",
        "A equação define o **retorno** $R(\\tau)$  como uma soma descontada das recompensas em uma trajetória,\n",
        "onde $γ ∈ [0, 1]$ é o **fator de desconto**.\n",
        "\n",
        "Então, o **objetivo** $J(τ )$ é simplesmente a expectativa dos retornos ao longo de muitas trajetórias,\n",
        "mostrado na equação:\n",
        "\n",
        "$$J(\\tau)=\\mathbb{E_{\\tau\\sim \\pi}}[R(\\tau)]=\\mathbb{E_{\\tau}}\\left[ \\sum_{t=0}^{T}\\gamma^t r_t \\right]$$\n",
        "\n",
        "O **retorno** $R(\\tau)$ é a soma das recompensas descontadas $\\gamma^t r_t$ em todos os intervalos de tempo $t = 0, . . . , T$.\n",
        "O **objetivo** $J(τ )$ é o **retorno médio** em muitos episódios. A expectativa leva em conta a estocasticidade nas ações e no ambiente — ou seja, em execuções repetidas, o retorno pode não ser sempre o mesmo. Maximizar o objetivo é o mesmo que maximizar o retorno."
      ],
      "metadata": {
        "id": "GmgsJjInwQgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O fator de desconto $ \\gamma \\in [0, 1] $ é uma variável importante que altera a forma como as recompensas futuras são valorizadas. Quanto menor $ \\gamma $, menor o peso dado às recompensas em etapas temporais futuras, tornando o processo \"de visão curta\". No caso extremo, com $ \\gamma = 0 $, o objetivo considera apenas a recompensa inicial $ r_0 $, como mostrado na equação abaixo:\n",
        "\n",
        "$$\n",
        "R(\\tau)_{\\gamma=0} = \\sum_{t=0}^{T} \\gamma^t r_t = r_0\n",
        "$$\n",
        "\n",
        "Quanto maior $ \\gamma $, maior o peso atribuído às recompensas em etapas temporais futuras: o objetivo se torna mais \"visionário\" ou \"de longo prazo\". Se $ \\gamma = 1 $, as recompensas de cada etapa temporal são ponderadas igualmente, como mostrado na equação abaixo:\n",
        "\n",
        "$$\n",
        "R(\\tau)_{\\gamma=1} = \\sum_{t=0}^{T} \\gamma^t r_t = \\sum_{t=0}^{T} r_t\n",
        "$$\n",
        "\n",
        "Para problemas com horizonte de tempo **infinito**, precisamos definir $ \\gamma < 1 $ para evitar que o objetivo se torne ilimitado. Para problemas com horizonte de tempo **finito**, $ \\gamma $ é um parâmetro importante, pois o problema pode se tornar mais ou menos difícil de resolver dependendo do fator de desconto que utilizamos."
      ],
      "metadata": {
        "id": "wwXvVhfUeF99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo definido o aprendizado por reforço como um Processo de Decisão de Markov (MDP) e estabelecido o objetivo, podemos agora expressar o ciclo de controle do aprendizado por reforço da figura abaixo\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1Wta1_jMOQ84pyWiTyiDro0EbsRpr2_YN' width=400>\n",
        "</center>\n",
        "\n",
        "como um ciclo de controle de MDP no no Algoritmo 1.1.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1UdibQPHC1YdBjTC9dEzv-sWQupYxOpNk' width=400>\n",
        "</center>"
      ],
      "metadata": {
        "id": "Jvxe6jnxhpZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algoritmo 1.1: Ciclo de Controle de MDP\n",
        "```text\n",
        "1: Dado um ambiente (env) e um agente:\n",
        "2: para episódio = 0, ..., MAX_EPISODE faça\n",
        "3:     estado = env.reset()\n",
        "4:     agent.reset()\n",
        "5:     para t = 0, ..., T faça\n",
        "6:         ação = agent.act(estado)\n",
        "7:         estado, recompensa = env.step(ação)\n",
        "8:         agent.update(ação, estado, recompensa)\n",
        "9:         se env.done() então\n",
        "10:            interrompa\n",
        "11:        fim se\n",
        "12:    fim para\n",
        "13: fim para\n",
        "```\n",
        "\n",
        "Esse ciclo descreve o processo iterativo de interação entre um agente e um ambiente em um contexto de aprendizado por reforço modelado como um MDP. Aqui estão os passos principais:\n",
        "\n",
        "1. O ambiente e o agente são inicializados.  \n",
        "2. Para cada episódio, o estado inicial do ambiente é resetado, e o agente também é reinicializado.  \n",
        "3. Em cada passo de tempo $ t $:\n",
        "   - O agente escolhe uma ação com base no estado atual.\n",
        "   - O ambiente avança um passo, retornando um novo estado e uma recompensa.\n",
        "   - O agente atualiza suas informações com base na ação tomada, no novo estado e na recompensa recebida.  \n",
        "4. O ciclo é encerrado se o ambiente indicar que o episódio terminou (`env.done()`).\n",
        "\n",
        "O Algoritmo 1.1 descreve as interações entre um agente e um ambiente ao longo de muitos episódios e passos de tempo. No início de cada episódio, o ambiente e o agente são reinicializados (linhas 3–4). Durante a reinicialização, o ambiente gera um estado inicial. Em seguida, inicia-se a interação: o agente produz uma ação com base no estado (linha 6), e o ambiente gera o próximo estado e a recompensa com base na ação tomada (linha 7), avançando para o próximo passo de tempo.  \n",
        "\n",
        "O ciclo **agent.act-env.step** continua até que o **número máximo** de passos de tempo $T$ seja alcançado ou o ambiente indique a terminação do episódio.  \n",
        "\n",
        "Além disso, observa-se um novo componente no algoritmo: **agent.update** (linha 8), que encapsula o algoritmo de aprendizado do agente. Esse método coleta dados e realiza o aprendizado internamente ao longo de múltiplos passos de tempo e episódios, com o objetivo de maximizar a função objetivo.\n",
        "\n",
        "Este algoritmo é genérico e pode ser adaptado para diferentes métodos de aprendizado por reforço, como Q-learning ou aprendizado baseado em políticas."
      ],
      "metadata": {
        "id": "6rFtT93ukEpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Função de Valor\n",
        "\n",
        "A **função de valor** representa a qualidade de longo prazo de um estado. Ela é o **retorno acumulado esperado no futuro** caso o agente inicie em um estado específico. Enquanto a recompensa mede o desempenho imediato, a função de valor avalia o desempenho no longo prazo.  \n",
        "\n",
        "- Uma **alta recompensa** imediata não garante uma **alta função de valor**, assim como uma **baixa recompensa** não significa necessariamente uma **baixa função de valor**.  \n",
        "- A função de valor pode ser definida de duas formas:\n",
        "  1. **Função de valor de estado** ($ V(s) $): mede o valor de estar em um estado $ s $.\n",
        "  2. **Função de valor de ação** ($ Q(s, a) $): mede o valor de realizar uma ação $ a $ em um estado $ s $.\n",
        "\n",
        "Essas funções são fundamentais para orientar o agente na escolha de ações que maximizem o retorno esperado ao longo do tempo.\n",
        "\n",
        "O diagrama mencionado mostra:\n",
        "- **Valores finais dos estados** (lado esquerdo): indicando o valor de longo prazo associado a cada estado.  \n",
        "- **Política ótima correspondente** (lado direito): destacando as ações que o agente deve tomar para maximizar o valor acumulado a partir de cada estado.\n"
      ],
      "metadata": {
        "id": "E9k0wr3RvV5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos detalhar as funções $ V(s) $ e $ Q(s, a) $:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Função de Valor de Estado ($ V(s) $)**\n",
        "\n",
        "A função de valor de estado, $ V(s) $, mede o retorno esperado ao começar em um estado $ s $ e seguir uma política $ \\pi $ (que define quais ações o agente tomará em cada estado). Matematicamente:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\mid s_0 = s \\right]\n",
        "$$\n",
        "\n",
        "- **$ r_t $**: recompensa recebida no passo de tempo $ t $.\n",
        "- **$ \\gamma $**: fator de desconto ($ 0 \\leq \\gamma < 1 $), que reduz a importância de recompensas futuras.\n",
        "- **$ \\mathbb{E}_\\pi $**: expectativa calculada com base na política $ \\pi $.\n",
        "\n",
        "**Interpretação**: $ V(s) $ responde à pergunta: \"Qual é o retorno esperado a partir do estado $ s $ seguindo a política $ \\pi $?\"\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Função de Valor de Ação ($ Q(s, a) $)**\n",
        "\n",
        "A função de valor de ação, $ Q(s, a) $, mede o retorno esperado ao tomar uma ação $ a $ em um estado $ s $ e, a partir daí, seguir a política $ \\pi $. Sua definição é:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\mid s_0 = s, a_0 = a \\right]\n",
        "$$\n",
        "\n",
        "**Interpretação**: $ Q(s, a) $ responde à pergunta: \"Qual é o retorno esperado ao executar a ação $ a $ no estado $ s $ e depois seguir a política $ \\pi $?\"\n",
        "\n",
        "---\n",
        "\n",
        "### Diferença entre $ V(s) $ e $ Q(s, a) $\n",
        "\n",
        "- $ V(s) $: avalia o **estado** considerando que as ações futuras seguem uma política $ \\pi $.  \n",
        "- $ Q(s, a) $: avalia uma **ação específica** em um estado, antes de seguir a política $ \\pi $.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Política Ótima e Relação com as Funções de Valor**\n",
        "\n",
        "A política ótima $ \\pi^* $ é aquela que maximiza o retorno esperado. Ela pode ser obtida das funções de valor:\n",
        "\n",
        "1. Para $ V^*(s) $:  \n",
        "   $$\n",
        "   V^*(s) = \\max_a Q^*(s, a)\n",
        "   $$\n",
        "   Aqui, $ V^*(s) $ representa o valor ótimo do estado $ s $, calculado pela melhor ação $ a $.\n",
        "\n",
        "2. Para $ Q^*(s, a) $:  \n",
        "   $$\n",
        "   Q^*(s, a) = r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^*(s')\n",
        "   $$\n",
        "   Onde:\n",
        "   - $ r(s, a) $: recompensa imediata ao tomar $ a $ em $ s $.\n",
        "   - $ P(s' \\mid s, a) $: probabilidade de transitar para $ s' $ ao tomar $ a $ em $ s $.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Exemplo Prático**\n",
        "\n",
        "Imagine um tabuleiro onde o agente deve chegar a um objetivo:\n",
        "\n",
        "1. **$ V(s) $**: o valor de cada célula no tabuleiro, que representa a recompensa esperada ao começar naquela célula e seguir a melhor política.\n",
        "2. **$ Q(s, a) $**: o valor de cada ação (ex: \"mover para cima\") em uma célula específica, representando a recompensa esperada ao realizar essa ação e depois seguir a melhor política.\n",
        "\n",
        "Com o tempo, o agente aprende esses valores para cada estado ou estado-ação, permitindo que ele escolha as melhores ações em cada situação."
      ],
      "metadata": {
        "id": "I3-F3KqyvsW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cálculo e Estimação de $ V(s) $ e $ Q(s, a) $\n",
        "\n",
        "Agora vamos detalhar como calcular ou estimar as funções de valor de estado $ V(s) $ e de ação $ Q(s, a) $ em diferentes contextos, como aprendizado por reforço. Existem duas abordagens principais: **cálculo exato (para problemas simples)** e **estimação iterativa (para problemas mais complexos)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Cálculo Exato: Equações de Bellman**\n",
        "\n",
        "#### a) Equação de Bellman para $ V(s) $\n",
        "A função de valor de estado pode ser definida recursivamente pela **Equação de Bellman**:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\sum_a \\pi(a \\mid s) \\left[ r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^\\pi(s') \\right]\n",
        "$$\n",
        "\n",
        "- **$ \\pi(a \\mid s) $**: probabilidade de escolher a ação $ a $ no estado $ s $ sob a política $ \\pi $.\n",
        "- **$ r(s, a) $**: recompensa esperada ao tomar a ação $ a $ no estado $ s $.\n",
        "- **$ P(s' \\mid s, a) $**: probabilidade de transição para o estado $ s' $ ao tomar a ação $ a $ no estado $ s $.\n",
        "\n",
        "---\n",
        "\n",
        "#### b) Equação de Bellman para $ Q(s, a) $\n",
        "A função de valor de ação também pode ser definida recursivamente:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\sum_{a'} \\pi(a' \\mid s') Q^\\pi(s', a')\n",
        "$$\n",
        "\n",
        "Aqui, a atualização inclui o valor das ações subsequentes $ Q^\\pi(s', a') $, ponderado pela política $ \\pi $.\n",
        "\n",
        "---\n",
        "\n",
        "#### c) Política Ótima e Equações Ótimas\n",
        "Quando buscamos a **política ótima** $ \\pi^* $, as equações são ajustadas para maximizar o valor:\n",
        "\n",
        "1. **Estado ótimo $ V^*(s) $:**\n",
        "   $$\n",
        "   V^*(s) = \\max_a \\left[ r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^*(s') \\right]\n",
        "   $$\n",
        "\n",
        "2. **Ação ótima $ Q^*(s, a) $:**\n",
        "   $$\n",
        "   Q^*(s, a) = r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\max_{a'} Q^*(s', a')\n",
        "   $$\n",
        "\n",
        "Essas equações fornecem a base para métodos de controle, como Iteração de Valores e Iteração de Políticas.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Métodos Iterativos para Estimação**\n",
        "\n",
        "Para problemas grandes ou contínuos, calcular diretamente $ V(s) $ ou $ Q(s, a) $ é inviável. Usamos métodos iterativos para aproximá-los.\n",
        "\n",
        "#### a) **Iteração de Valores**\n",
        "1. Inicie com $ V(s) $ arbitrário para todos os estados.\n",
        "2. Atualize iterativamente usando a Equação de Bellman:\n",
        "   $$\n",
        "   V_{k+1}(s) = \\max_a \\left[ r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V_k(s') \\right]\n",
        "   $$\n",
        "3. Pare quando $ V(s) $ convergir.\n",
        "\n",
        "---\n",
        "\n",
        "#### b) **Iteração de Políticas**\n",
        "1. Inicie com uma política arbitrária $ \\pi $.\n",
        "2. **Avaliação da Política**:\n",
        "   - Calcule $ V^\\pi(s) $ para a política atual.\n",
        "3. **Melhoria da Política**:\n",
        "   - Atualize $ \\pi $ escolhendo a melhor ação:\n",
        "     \n",
        "     $$\\pi'(s) = \\arg\\max_a \\left[ r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^\\pi(s')\\right]$$\n",
        "     \n",
        "4. Repita até que $ \\pi $ seja estável.\n",
        "\n",
        "---\n",
        "\n",
        "#### c) **Q-Learning**\n",
        "Quando o modelo do ambiente não é conhecido, usamos métodos baseados em amostras, como **Q-Learning**:\n",
        "\n",
        "1. Inicialize $ Q(s, a) $ arbitrariamente.\n",
        "2. Em cada interação, atualize $ Q(s, a) $:\n",
        "   $$\n",
        "   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "   $$\n",
        "   - $ \\alpha $: taxa de aprendizado.\n",
        "   - $ r $: recompensa observada.\n",
        "   - $ \\gamma $: fator de desconto.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Resumo e Conexões**\n",
        "\n",
        "- **Iteração de Valores** e **Iteração de Políticas** são usadas quando conhecemos $ P(s' \\mid s, a) $ e $ r(s, a) $.  \n",
        "- **Q-Learning** é útil quando o ambiente é desconhecido e a interação direta é necessária.  \n",
        "- Ambas as funções ($ V(s) $ e $ Q(s, a) $) ajudam o agente a tomar decisões que maximizam o retorno esperado."
      ],
      "metadata": {
        "id": "z-glvamzwxuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q-Table of Q-Learning\n",
        "A fórmula de atualização da **Q-table** vem da **Equação de Bellman**, que descreve a relação recursiva entre o valor atual de um estado ou ação e os valores futuros esperados. A ideia central é que o valor $Q(s, a)$ de uma ação em um estado é igual à recompensa imediata esperada mais o valor esperado das recompensas futuras, descontadas pelo fator $\\gamma$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Origem: Equação de Bellman para Valores Q**\n",
        "\n",
        "A função Q ($Q(s, a)$) representa a recompensa acumulada esperada ao tomar a ação $a$ no estado $s$ e, a partir daí, seguir a política ótima. A relação de Bellman é:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E} \\left[ R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a') \\right]\n",
        "$$\n",
        "\n",
        "#### **Explicação dos termos:**\n",
        "1. $R(s, a, s')$: A recompensa imediata ao executar a ação $a$ no estado $s$ e transitar para $s'$.\n",
        "2. $\\max_{a'} Q^*(s', a')$: O valor ótimo esperado das recompensas futuras, assumindo que o agente age de forma ótima após transitar para o estado $s'$.\n",
        "3. $\\gamma$: O fator de desconto ($0 \\leq \\gamma \\leq 1$), que controla a importância das recompensas futuras.\n",
        "\n",
        "Essa equação diz que o valor $Q^*(s, a)$ pode ser decomposto em duas partes:\n",
        "- A recompensa imediata.\n",
        "- A melhor recompensa esperada a partir do próximo estado.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Fórmula de Atualização da Q-table**\n",
        "\n",
        "No **Q-learning**, uma técnica de aprendizado por reforço, o agente utiliza interações com o ambiente para estimar a função $Q(s, a)$ sem conhecimento prévio das funções de transição ($P(s'|s, a)$) ou recompensa ($R(s, a, s')$).\n",
        "\n",
        "A atualização da Q-table segue a seguinte fórmula:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\gets Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "#### **Derivação:**\n",
        "1. **Definir a meta:** A equação de Bellman nos dá o valor \"correto\" para $Q(s, a)$:\n",
        "   $$\n",
        "   Q_{\\text{target}} = R + \\gamma \\max_{a'} Q(s', a')\n",
        "   $$\n",
        "\n",
        "2. **Erro temporal:** O erro entre o valor atual de $Q(s, a)$ e o valor-alvo é:\n",
        "   $$\n",
        "   \\delta = Q_{\\text{target}} - Q(s, a)\n",
        "   $$\n",
        "\n",
        "3. **Atualizar gradualmente:** Em vez de substituir diretamente $Q(s, a)$ pelo valor-alvo (o que seria instável), usamos um fator de aprendizado ($\\alpha$) para fazer uma atualização parcial:\n",
        "   $$\n",
        "   Q(s, a) \\gets Q(s, a) + \\alpha \\delta\n",
        "   $$\n",
        "\n",
        "Substituindo $Q_{\\text{target}}$, obtemos:\n",
        "$$\n",
        "Q(s, a) \\gets Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Componentes da Fórmula de Atualização**\n",
        "\n",
        "1. **Valor atual ($Q(s, a)$)**: Representa o conhecimento atual do agente sobre a qualidade da ação $a$ no estado $s$.\n",
        "\n",
        "2. **Recompensa imediata ($R$)**: Feedback direto obtido ao executar a ação.\n",
        "\n",
        "3. **Estimativa do futuro ($\\max_{a'} Q(s', a')$)**: Melhor recompensa futura esperada a partir do próximo estado $s'$.\n",
        "\n",
        "4. **Fator de aprendizado ($\\alpha$)**: Controla a taxa de atualização. Valores altos tornam o aprendizado rápido, mas menos estável; valores baixos tornam o aprendizado lento, mas mais robusto.\n",
        "\n",
        "5. **Fator de desconto ($\\gamma$)**: Controla a importância das recompensas futuras em relação às recompensas imediatas.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Intuição da Fórmula**\n",
        "\n",
        "- O valor atual $Q(s, a)$ é ajustado para se aproximar do valor-alvo $Q_{\\text{target}}$, que é baseado na recompensa imediata $R$ e na melhor estimativa futura $\\gamma \\max_{a'} Q(s', a')$.\n",
        "- O aprendizado ocorre iterativamente, permitindo ao agente melhorar sua estimativa de $Q(s, a)$ à medida que interage com o ambiente.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Exemplos**\n",
        "\n",
        "Imagine um rato em um labirinto onde:\n",
        "- Estados ($s$): Células do labirinto.\n",
        "- Ações ($a$): Movimentos (cima, baixo, esquerda, direita).\n",
        "- Recompensas ($R$): $+10$ para encontrar o queijo, $-1$ para cada movimento.\n",
        "\n",
        "Se o rato está na célula $s = (1, 1)$, move-se para a célula $s' = (1, 2)$, e ganha uma recompensa $R = -1$, a atualização seria:\n",
        "\n",
        "$$\n",
        "Q((1, 1), \\text{direita}) \\gets Q((1, 1), \\text{direita}) + \\alpha \\left[ -1 + \\gamma \\max_{a'} Q((1, 2), a') - Q((1, 1), \\text{direita}) \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Se precisar de uma implementação prática ou visualização, posso ajudar!"
      ],
      "metadata": {
        "id": "9NKh6Rot9lqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aplicações na prática\n",
        "Aqui estão alguns exemplos práticos de Processos de Decisão de Markov (MDPs) em diferentes contextos:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Robótica: Navegação em Ambientes**\n",
        "**Problema**: Um robô precisa se mover de uma posição inicial para um objetivo em um grid 2D com obstáculos.\n",
        "\n",
        "- **Estados ($S$)**: Cada célula do grid é um estado possível.\n",
        "- **Ações ($A$)**: Mover para cima, para baixo, para a esquerda ou para a direita.\n",
        "- **Transições ($P(s'|s, a)$)**: Existe uma probabilidade de $80\\%$ de o robô se mover na direção desejada e $20\\%$ de deslizar para uma célula vizinha.\n",
        "- **Recompensa ($R(s, a)$)**:\n",
        "  - $+10$ ao alcançar o objetivo.\n",
        "  - $-1$ por cada movimento realizado.\n",
        "  - $-100$ se colidir com um obstáculo.\n",
        "\n",
        "**Solução**: O robô utiliza algoritmos de aprendizado por reforço, como Q-Learning, para aprender a melhor política de navegação.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Jogos: Agente para Jogos de Tabuleiro**\n",
        "**Problema**: Desenvolver um agente para jogar xadrez ou um jogo como Tic-Tac-Toe.\n",
        "\n",
        "- **Estados ($S$)**: Configurações possíveis do tabuleiro.\n",
        "- **Ações ($A$)**: Jogadas válidas em cada estado.\n",
        "- **Transições ($P(s'|s, a)$)**: A transição para o próximo estado depende das regras do jogo e das ações do adversário.\n",
        "- **Recompensa ($R(s, a)$)**:\n",
        "  - $+1$ por vencer o jogo.\n",
        "  - $-1$ por perder.\n",
        "  - $0$ para empates ou estados intermediários.\n",
        "\n",
        "**Solução**: Usar RL baseado em modelo (como AlphaZero) para aprender estratégias a partir de simulações.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Saúde: Planejamento de Tratamentos Médicos**\n",
        "**Problema**: Otimizar o tratamento para pacientes com doenças crônicas, como diabetes ou câncer.\n",
        "\n",
        "- **Estados ($S$)**: Representam a condição atual do paciente (por exemplo, níveis de glicose no sangue ou estágio do tumor).\n",
        "- **Ações ($A$)**: Decisões médicas, como ajustar medicação, recomendar exercícios ou alterar a dieta.\n",
        "- **Transições ($P(s'|s, a)$)**: As transições refletem como o estado do paciente muda com base no tratamento aplicado.\n",
        "- **Recompensa ($R(s, a)$)**: Maximizar o bem-estar do paciente e minimizar os custos do tratamento.\n",
        "\n",
        "**Solução**: Usar MDPs para modelar o problema e algoritmos de RL para encontrar políticas personalizadas para cada paciente.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Finanças: Otimização de Portfólio**\n",
        "**Problema**: Determinar quanto investir em diferentes ativos para maximizar os retornos ao longo do tempo.\n",
        "\n",
        "- **Estados ($S$)**: Distribuição atual do portfólio e preços dos ativos.\n",
        "- **Ações ($A$)**: Comprar, vender ou manter ativos específicos.\n",
        "- **Transições ($P(s'|s, a)$)**: Modelam a evolução dos preços dos ativos e do portfólio.\n",
        "- **Recompensa ($R(s, a)$)**: Lucro ou perda financeira em cada período.\n",
        "\n",
        "**Solução**: Usar RL para identificar estratégias que balanceiem retorno e risco.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Sistemas Autônomos: Controle de Tráfego**\n",
        "**Problema**: Controlar os sinais de trânsito em uma cidade para minimizar congestionamentos.\n",
        "\n",
        "- **Estados ($S$)**: Condição atual do tráfego em cada interseção (número de veículos nas filas).\n",
        "- **Ações ($A$)**: Ajustar a duração dos sinais (verde, vermelho, amarelo) em cada direção.\n",
        "- **Transições ($P(s'|s, a)$)**: Refletem a dinâmica do fluxo de veículos.\n",
        "- **Recompensa ($R(s, a)$)**: Penalidade proporcional ao tempo médio de espera dos veículos.\n",
        "\n",
        "**Solução**: Aplicar aprendizado profundo em combinação com RL (Deep RL) para controlar sinais em tempo real.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Gerenciamento de Recursos: Escalonamento de Servidores**\n",
        "**Problema**: Gerenciar a alocação de servidores em um data center para lidar com demandas variáveis.\n",
        "\n",
        "- **Estados ($S$)**: Número de servidores ativos e carga de trabalho atual.\n",
        "- **Ações ($A$)**: Ativar ou desativar servidores.\n",
        "- **Transições ($P(s'|s, a)$)**: Mudanças na carga de trabalho e estado dos servidores.\n",
        "- **Recompensa ($R(s, a)$)**:\n",
        "  - Penalidades para consumo excessivo de energia.\n",
        "  - Recompensas para tempos de resposta rápidos.\n",
        "\n",
        "**Solução**: Modelar como um MDP e usar RL para aprender estratégias de escalonamento eficiente.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U9UoRBLAspOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Referências\n",
        "1. \"Markov Decision Processes: Discrete Stochastic Dynamic Programming\" de Martin L. Puterman.\n",
        "1. Laura Graesser, Wah Loon Keng. **Foundations of Deep Reinforcement Learning - Theory and Practice in Python**. Pearson Addison-Wesley. 2023.\n",
        "2. \"Reinforcement Learning: An Introduction\" de Richard S. Sutton e Andrew G. Barto.\n",
        "3. [\"MDP Tutorial\" de Stanford University](https://ai.stanford.edu/~gwthomas/notes/mdps.pdf)\n",
        "4. https://gymnasium.farama.org/introduction/basic_usage/\n",
        "5. [Understanding the Markov Decision Process (MDP)](https://builtin.com/machine-learning/markov-decision-process)"
      ],
      "metadata": {
        "id": "vhcdxS3cS5_O"
      }
    }
  ]
}