{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6HnZEzQHWOrN/g1gpoBy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonimerMelo/Reinforcement-Learning/blob/Q-Learning/Q_learning_introduction_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reinforcement Learning Q-Learning Algorithm"
      ],
      "metadata": {
        "id": "CHFyLaYyyocb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q-learning: Algoritmo de Aprendizado por Reforço**\n",
        "\n",
        "**Q-learning** é um algoritmo de aprendizado por reforço **off-policy** (ou seja, o agente aprende uma política ótima independentemente da política seguida durante a execução) que visa encontrar a **política ótima** em um ambiente com transições estocásticas e recompensas desconhecidas. O objetivo é aprender o valor da função de ação-valor $ Q(s, a) $, que define o valor de tomar uma ação $ a $ no estado $ s $ e seguir uma política ótima após isso.\n",
        "\n",
        "### **Objetivo do Q-learning**\n",
        "A meta do Q-learning é estimar a função de valor de ação ótima $ Q^*(s, a) $ para encontrar a política ótima $ \\pi^*(s) $, que é dada por:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
        "$$\n",
        "\n",
        "Onde:\n",
        "\n",
        "- $ Q^*(s, a) $ é o valor da ação $ a $ no estado $ s $, que reflete o retorno esperado ao tomar a ação $ a $ em $ s $ e, a partir daí, seguir a política ótima.\n",
        "\n",
        "### **Fórmula de Atualização do Q-learning**\n",
        "Durante a execução, o agente atualiza a função de valor de ação $ Q(s, a) $ com base nas experiências coletadas. A fórmula de atualização do Q-learning é:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "Onde:\n",
        "\n",
        "- $ Q(s, a) $: valor atual da ação $ a $ no estado $ s $.\n",
        "- $ \\alpha $: taxa de aprendizado (quanto o agente deve ajustar o valor estimado com base em novas informações).\n",
        "- $ r $: recompensa imediata recebida após tomar a ação $ a $ no estado $ s $.\n",
        "- $ \\gamma $: fator de desconto (quanto o agente valoriza recompensas futuras em relação às recompensas imediatas).\n",
        "- $ \\max_{a'} Q(s', a') $: o valor máximo de $ Q(s', a') $ no próximo estado $ s' $, que indica o melhor valor possível de ação para o próximo estado.\n",
        "\n",
        "### **Passos do Algoritmo Q-learning**\n",
        "Aqui estão os passos principais do algoritmo Q-learning:\n",
        "\n",
        "1. **Inicialização**:\n",
        "   - Inicialize a função de valor de ação $ Q(s, a) $ arbitrariamente (normalmente com valores próximos de zero para todos os $ s $ e $ a $).\n",
        "   - Defina a taxa de aprendizado $ \\alpha $ e o fator de desconto $ \\gamma $.\n",
        "   - Inicialize o número máximo de episódios e o número máximo de passos por episódio.\n",
        "\n",
        "2. **Interação com o Ambiente**:\n",
        "   - Para cada episódio:\n",
        "     1. **Estado Inicial**: Reinicie o ambiente e defina o estado inicial $ s $.\n",
        "     2. **Execução do Passo de Tempo**: Para cada passo de tempo $ t $:\n",
        "        - O agente escolhe uma ação $ a $ em $ s $ com base na **política de exploração-exploração** (geralmente uma política $ \\epsilon $-greedy):\n",
        "          - Com probabilidade $ 1 - \\epsilon $, escolha a ação $ a = \\arg\\max_a Q(s, a) $ (exploitation).\n",
        "          - Com probabilidade $ \\epsilon $, escolha uma ação aleatória (exploration).\n",
        "        - O agente executa a ação $ a $, observa a recompensa $ r $ e o novo estado $ s' $.\n",
        "        - O agente atualiza $ Q(s, a) $ com a fórmula de atualização do Q-learning.\n",
        "        - Atualize o estado $ s \\leftarrow s' $.\n",
        "        - Se o estado $ s' $ for terminal, pare o episódio.\n",
        "\n",
        "3. **Repetição**:\n",
        "   - Repita o processo para múltiplos episódios até que a função $ Q(s, a) $ converja para $ Q^*(s, a) $.\n",
        "\n",
        "### **Pseudocódigo do Q-learning**\n",
        "\n",
        "```python\n",
        "# Inicialização\n",
        "Q = {}  # Função de valor de ação (arbitrário para todos os estados e ações)\n",
        "alpha = 0.1  # Taxa de aprendizado\n",
        "gamma = 0.9  # Fator de desconto\n",
        "epsilon = 0.1  # Probabilidade de exploração\n",
        "MAX_EPISODES = 1000  # Número máximo de episódios\n",
        "MAX_STEPS = 100  # Número máximo de passos por episódio\n",
        "\n",
        "for episode in range(MAX_EPISODES):\n",
        "    estado = ambiente.reset()  # Reiniciar o ambiente e obter estado inicial\n",
        "    for t in range(MAX_STEPS):\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            ação = ambiente.ação_aleatória()  # Exploração (escolher ação aleatória)\n",
        "        else:\n",
        "            ação = max(Q[estado], key=Q[estado].get)  # Exploração (escolher a melhor ação)\n",
        "\n",
        "        novo_estado, recompensa, feito = ambiente.step(ação)  # Executar a ação no ambiente\n",
        "        # Atualização do valor Q usando a fórmula de atualização do Q-learning\n",
        "        Q[estado][ação] += alpha * (recompensa + gamma * max(Q[novo_estado].values()) - Q[estado][ação])\n",
        "\n",
        "        estado = novo_estado  # Mover para o próximo estado\n",
        "\n",
        "        if feito:\n",
        "            break  # Terminar o episódio se o estado for terminal\n",
        "```\n"
      ],
      "metadata": {
        "id": "B0N6g1Hwx_Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exploration vs Exploitation**\n",
        "`Obs. importante: Não existe tradução para 'exploitation' no português. É traduzido como 'exploração' e pode confundir com 'exploration', que tem a mesma tradução!`\n",
        "\n",
        "Um dos desafios do Q-learning é o dilema entre **Exploration** (tentar novas ações) e **Exploitation** (usar o conhecimento adquirido para maximizar a recompensa). Para equilibrar isso, muitas vezes usa-se uma política **$ \\epsilon $-greedy**, onde o agente:\n",
        "\n",
        "- Com probabilidade $ 1 - \\epsilon $, escolhe a melhor ação $ a = \\arg\\max_a Q(s, a) $ (Exploitation).\n",
        "- Com probabilidade $ \\epsilon $, escolhe uma ação aleatória (Exploration).\n",
        "\n",
        "A $ \\epsilon $ geralmente começa com um valor relativamente alto e decai lentamente durante o treinamento, para que o agente explore mais no início e depois explore mais.\n",
        "O dilema **exploração vs exploração** (ou **exploration vs exploitation**) é um dos conceitos centrais no aprendizado por reforço. Ele descreve o trade-off entre:\n",
        "\n",
        "- **Exploration**: Tentar novas ações para descobrir informações sobre o ambiente que ainda não foram aprendidas.\n",
        "- **Exploitation**: Usar o conhecimento adquirido até o momento (ou seja, o que já foi aprendido) para maximizar a recompensa.\n",
        "\n",
        "### **Exploração (Exploration)**\n",
        "\n",
        "- **Objetivo**: O objetivo da exploração é **descobrir novas informações** sobre o ambiente. Isso é importante porque o agente pode não saber inicialmente quais ações levam a boas recompensas, e a exploração permite que ele descubra essas ações.\n",
        "- **Exemplo**: Em um tabuleiro de xadrez, se um jogador tentar movimentos nunca antes testados, ele está explorando novas estratégias.\n",
        "- **Benefício**: A exploração ajuda a evitar que o agente se prenda a uma solução subótima, garantindo que ele tenha a chance de descobrir a política ótima.\n",
        "- **Risco**: A exploração pode levar a ações que resultam em recompensas baixas ou até negativas, pois o agente está tentando algo novo sem saber o resultado.\n",
        "\n",
        "### **Exploração (Exploitation)**\n",
        "- **Objetivo**: O objetivo da exploitation é **usar o conhecimento já adquirido** para maximizar a recompensa. Quando o agente explora, ele escolhe as ações que ele sabe que são boas com base na experiência passada.\n",
        "- **Exemplo**: Se um jogador de xadrez já aprendeu que uma determinada jogada é muito forte, ele escolheria essa jogada sempre que possível.\n",
        "- **Benefício**: A exploração maximiza a recompensa imediata, pois o agente sempre escolhe a ação que já levou a boas recompensas no passado.\n",
        "- **Risco**: Se o agente sempre explorar (usar as mesmas ações), ele pode perder a oportunidade de encontrar uma estratégia melhor (política ótima) que ainda não foi explorada.\n",
        "\n",
        "### **O Dilema**\n",
        "O **dilema** ocorre porque, se o agente se concentrar apenas em explorar ou apenas em explorar, ele pode não alcançar a melhor solução:\n",
        "\n",
        "- Se ele **explorar demais**, pode perder recompensas mais altas, já que ele pode continuar tentando ações sem sucesso.\n",
        "- Se ele **explorar pouco**, pode se fixar em uma política subótima, já que ele não está tentando novas possibilidades que poderiam ser melhores.\n",
        "\n",
        "### **Como Gerenciar o Dilema: Estratégias**\n",
        "\n",
        "A maneira de gerenciar o dilema é usar uma política que balanceie exploração e exploração. A mais comum é a **política $ \\epsilon $-greedy**.\n",
        "\n",
        "#### **Política $ \\epsilon $-greedy**\n",
        "- A política $ \\epsilon $-greedy permite que o agente **explore** com uma probabilidade $ \\epsilon $ e **explore** com uma probabilidade $ 1 - \\epsilon $.\n",
        "- **Exploração**: Com probabilidade $ \\epsilon $, o agente escolhe uma ação aleatória.\n",
        "- **Exploitation**: Com probabilidade $ 1 - \\epsilon $, o agente escolhe a melhor ação com base nas estimativas de $ Q(s, a) $ (ou a política atual).\n",
        "\n",
        "**Exemplo**:\n",
        "- Se $ \\epsilon = 0.1 $, o agente explora (escolhe uma ação aleatória) 10% das vezes e explora (escolhe a melhor ação) 90% das vezes.\n",
        "\n",
        "#### **Diminuição de $ \\epsilon $**\n",
        "\n",
        "Uma abordagem comum é **diminuir $ \\epsilon $** ao longo do tempo. No início, o agente pode explorar mais, pois ele não tem informações suficientes sobre o ambiente. Com o tempo, ele se concentra mais em explorar, à medida que adquire mais conhecimento sobre o ambiente.\n",
        "\n",
        "- **Exploração inicial**: $ \\epsilon $ começa com um valor alto (ex: 0.9), incentivando a exploração.\n",
        "- **Exploração final**: $ \\epsilon $ diminui gradualmente (ex: até 0.1 ou 0.01), permitindo que o agente explore mais.\n",
        "\n",
        "### **Exemplo Prático: Política $ \\epsilon $-greedy**\n",
        "\n",
        "Imagine um ambiente simples em que o agente tem 5 ações possíveis em um estado e quer aprender qual delas traz a maior recompensa. A política $ \\epsilon $-greedy poderia ser usada da seguinte forma:\n",
        "\n",
        "1. **Início**: O agente explora com $ \\epsilon = 0.9 $, então ele escolheria uma ação aleatória 90% das vezes e escolheria a melhor ação 10% das vezes.\n",
        "2. **Após alguns episódios**: O agente começa a entender quais ações levam a melhores recompensas. O valor de $ \\epsilon $ diminui gradualmente para 0.1.\n",
        "3. **Final**: O agente explora muito menos, preferindo explorar a melhor ação 90% das vezes.\n",
        "\n",
        "### **Outras Estratégias**\n",
        "\n",
        "Além da política $ \\epsilon $-greedy, existem outras estratégias para balancear exploração e exploração:\n",
        "\n",
        "1. **Boltzmann Exploration**:\n",
        "   - Em vez de escolher uma ação aleatoriamente com probabilidade $ \\epsilon $, a **exploração** é feita de forma probabilística com base em uma distribuição de Boltzmann, onde as ações com valores $ Q(s, a) $ mais altos são mais prováveis de serem escolhidas.\n",
        "   \n",
        "2. **Softmax**:\n",
        "   - Similar ao Boltzmann, o agente escolhe ações com base em probabilidades que dependem dos valores $ Q(s, a) $. Quanto maior o valor $ Q(s, a) $, maior a chance de escolher essa ação."
      ],
      "metadata": {
        "id": "FZwbPQglzq6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusão**\n",
        "- O Q-learning é uma técnica poderosa de aprendizado por reforço que permite ao agente aprender uma política ótima, mesmo sem conhecer o modelo do ambiente.\n",
        "- A atualização das funções de valor de ação $ Q(s, a) $ ao longo do tempo permite ao agente aprender quais ações são mais vantajosas em cada estado.\n",
        "- A política $ \\epsilon $-greedy é crucial para equilibrar exploração e exploração durante o aprendizado.\n",
        "\n",
        "O **dilema Exploration vs Exploitation** é fundamental no aprendizado por reforço, pois afeta como o agente aprende a tomar decisões. Um bom equilíbrio entre explorar novas ações e explorar as melhores ações conhecidas é essencial para encontrar a política ótima. A política $ \\epsilon $-greedy é uma solução simples e eficaz para esse dilema, especialmente quando $ \\epsilon $ é ajustado ao longo do tempo."
      ],
      "metadata": {
        "id": "rQ8W4Rig1jV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q-learning aplicado ao CartPole\n",
        "O **Q-learning aplicado ao CartPole** é um exemplo clássico de problema de controle em aprendizado por reforço. O CartPole é um ambiente de simulação onde o objetivo é controlar um carrinho que se move ao longo de uma linha reta, com um pólo (barra) invertida em cima. O objetivo é equilibrar o pólo na posição vertical o maior tempo possível.\n",
        "\n",
        "### Descrição do Problema CartPole\n",
        "\n",
        "No **CartPole** (do OpenAI Gym), a tarefa consiste em aprender a controlar o carrinho (com ações de \"mover para a esquerda\" ou \"mover para a direita\") para equilibrar o pólo. O estado do ambiente é descrito por 4 variáveis:\n",
        "\n",
        "1. **Posição do carrinho**: A posição do carrinho ao longo da linha.\n",
        "2. **Velocidade do carrinho**: A velocidade com que o carrinho se move.\n",
        "3. **Ângulo do pólo**: O ângulo entre a barra e a vertical.\n",
        "4. **Velocidade angular do pólo**: A taxa de variação do ângulo do pólo.\n",
        "\n",
        "O agente pode realizar uma das duas ações:\n",
        "- **0**: Mover para a esquerda.\n",
        "- **1**: Mover para a direita.\n",
        "\n",
        "O objetivo do Q-learning é aprender uma política ótima, ou seja, um conjunto de ações a tomar em cada estado para maximizar a recompensa total, que no caso do CartPole é o tempo que o pólo permanece equilibrado.\n",
        "\n",
        "### Q-learning no CartPole\n",
        "\n",
        "O Q-learning será usado para aprender uma **função de valor de ação** $ Q(s, a) $, que representa a qualidade de tomar uma ação $ a $ no estado $ s $. O Q-learning é um método **off-policy**, ou seja, o agente aprende a política ótima independentemente das ações que ele toma durante a execução.\n",
        "\n",
        "A **função de valor $ Q(s, a) $** é atualizada com base na fórmula de atualização do Q-learning:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "Onde:\n",
        "- $ s $ é o estado atual.\n",
        "- $ a $ é a ação tomada.\n",
        "- $ r $ é a recompensa recebida após a ação.\n",
        "- $ s' $ é o novo estado após a ação.\n",
        "- $ a' $ é a ação possível no novo estado $ s' $.\n",
        "- $ \\alpha $ é a taxa de aprendizado.\n",
        "- $ \\gamma $ é o fator de desconto.\n",
        "  \n",
        "### Passos para implementar Q-learning no CartPole\n",
        "\n",
        "1. **Inicializar a tabela $ Q(s, a) $**: Como o CartPole tem um espaço contínuo de estados, precisaremos discretizar os estados para armazenar os valores de $ Q(s, a) $.\n",
        "   \n",
        "2. **Escolher uma política de exploration/exploitation**: Uma política $ \\epsilon $-greedy pode ser usada, onde o agente escolhe a ação com base no valor $ Q(s, a) $ com maior probabilidade $ 1-\\epsilon $, e escolhe uma ação aleatória com probabilidade $ \\epsilon $.\n",
        "\n",
        "3. **Interagir com o ambiente**: Para cada episódio:\n",
        "   - O agente começa em um estado inicial.\n",
        "   - O agente escolhe uma ação de acordo com a política.\n",
        "   - A ação é executada, o novo estado e a recompensa são observados.\n",
        "   - A função de valor $ Q(s, a) $ é atualizada.\n",
        "   - O episódio termina quando o pólo cai ou o número máximo de passos é atingido.\n",
        "\n",
        "4. **Repetir até convergir**: Continuar o processo por um grande número de episódios até que a função $ Q(s, a) $ se estabilize, e o agente aprenda a política ótima.\n",
        "\n",
        "### Exemplo de Código para Q-learning no CartPole\n",
        "\n",
        "Abaixo está um exemplo simples de implementação de Q-learning para o problema CartPole usando o OpenAI Gym.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "\n",
        "# Criar o ambiente CartPole\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Definir parâmetros\n",
        "alpha = 0.1  # Taxa de aprendizado\n",
        "gamma = 0.99  # Fator de desconto\n",
        "epsilon = 0.1  # Taxa de exploração\n",
        "episodes = 1000  # Número de episódios\n",
        "max_steps = 200  # Número máximo de passos por episódio\n",
        "\n",
        "# Discretizar o espaço de estados (aproximação)\n",
        "n_bins = 10  # Número de bins para discretização dos estados\n",
        "state_bins = [np.linspace(-x, x, n_bins) for x in [4.8, 5.0, 0.418, 5.0]]  # Limites do estado\n",
        "q_table = np.zeros([len(state_bins[0]) + 1, len(state_bins[1]) + 1, len(state_bins[2]) + 1, len(state_bins[3]) + 1, 2])  # Tabela Q\n",
        "\n",
        "# Função para discretizar o estado contínuo\n",
        "def discretize_state(state):\n",
        "    state_discretized = []\n",
        "    for i in range(len(state)):\n",
        "        state_discretized.append(np.digitize(state[i], state_bins[i]) - 1)\n",
        "    return tuple(state_discretized)\n",
        "\n",
        "# Política epsilon-greedy\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return env.action_space.sample()  # Exploração\n",
        "    else:\n",
        "        return np.argmax(q_table[state])  # Exploração\n",
        "\n",
        "# Treinamento\n",
        "for episode in range(episodes):\n",
        "    state = discretize_state(env.reset())  # Estado inicial\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)  # Executar ação\n",
        "        next_state = discretize_state(next_state)\n",
        "\n",
        "        # Atualização da tabela Q\n",
        "        q_table[state][action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state][action])\n",
        "\n",
        "        state = next_state  # Atualizar o estado\n",
        "\n",
        "        total_reward += reward  # Acumular recompensa\n",
        "\n",
        "        if done:\n",
        "            break  # Fim do episódio\n",
        "\n",
        "    print(f'Episódio {episode + 1}/{episodes} - Recompensa: {total_reward}')\n",
        "\n",
        "# Avaliação final\n",
        "total_reward = 0\n",
        "for _ in range(100):  # Testar o agente após o treinamento\n",
        "    state = discretize_state(env.reset())\n",
        "    for step in range(max_steps):\n",
        "        action = np.argmax(q_table[state])  # Escolher ação com maior valor Q\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        state = discretize_state(next_state)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "print(f'Recompensa média após treinamento: {total_reward / 100}')\n",
        "```\n",
        "\n",
        "### Explicação do Código\n",
        "\n",
        "- **Discretização dos estados**: Como o CartPole tem um espaço contínuo de estados, usamos a função `discretize_state` para dividir cada variável de estado (como a posição do carrinho ou o ângulo do pólo) em bins discretos. Isso ajuda a armazenar e acessar os valores de $ Q(s, a) $ na tabela.\n",
        "  \n",
        "- **Tabela Q**: A tabela $ Q $ é uma matriz que mapeia pares de estado e ação para valores de recompensa. O tamanho da tabela depende do número de bins que usamos para discretizar cada uma das 4 variáveis do estado.\n",
        "\n",
        "- **Política $ \\epsilon $-greedy**: A função `choose_action` escolhe uma ação com base em uma política $ \\epsilon $-greedy, explorando ações aleatórias com probabilidade $ \\epsilon $ e explorando a melhor ação com probabilidade $ 1-\\epsilon $.\n",
        "\n",
        "- **Treinamento**: O agente interage com o ambiente durante um número fixo de episódios e atualiza a tabela $ Q $ após cada ação tomada.\n",
        "\n",
        "### Considerações Finais\n",
        "\n",
        "- **Desafios**: O CartPole é um ambiente com um espaço de estados contínuo e, portanto, a discretização pode afetar a qualidade do aprendizado. Em problemas mais complexos, técnicas como redes neurais (usando Deep Q-Learning) podem ser usadas para aproximar a função de valor $ Q(s, a) $ sem a necessidade de discretizar o espaço de estados.\n",
        "  \n",
        "- **Exploração e Exploração**: A política $ \\epsilon $-greedy é fundamental para garantir que o agente explore diferentes possibilidades antes de se fixar nas ações mais vantajosas.\n"
      ],
      "metadata": {
        "id": "vVjcEHQo2cZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tabela Q-Learning (Q-Table)\n",
        "A **Tabela Q-Learning (Q-Table)** é a estrutura de dados fundamental no algoritmo de aprendizado por reforço **Q-Learning**. Ela é utilizada para armazenar e atualizar os valores de qualidade (valores Q) associados a cada estado e ação. Esses valores representam a **recompensa** esperada que o agente pode obter a partir de um estado ao executar uma determinada ação.\n",
        "\n",
        "---\n",
        "\n",
        "### **Estrutura da Q-Table**\n",
        "- A Q-Table é uma matriz (ou tensor) onde:\n",
        "  - **Linhas (Estados):** Representam os estados possíveis do ambiente.\n",
        "  - **Colunas (Ações):** Representam as ações possíveis que o agente pode tomar.\n",
        "  - Cada célula da tabela (**Q(s, a)**) contém o valor Q, que é a **estimativa da recompensa futura** esperada para o par estado-ação $(s, a)$.\n",
        "\n",
        "Por exemplo:\n",
        "- Para um ambiente com 3 estados ($S = \\{s_0, s_1, s_2\\}$) e 2 ações ($A = \\{a_0, a_1\\}$), a Q-Table teria o formato:\n",
        "\n",
        "| Estado | $a_0$  | $a_1$  |\n",
        "|--------|----------|----------|\n",
        "| $s_0$ | $Q(s_0, a_0)$ | $Q(s_0, a_1)$ |\n",
        "| $s_1$ | $Q(s_1, a_0)$ | $Q(s_1, a_1)$ |\n",
        "| $s_2$ | $Q(s_2, a_0)$ | $Q(s_2, a_1)$ |\n",
        "\n",
        "---\n",
        "\n",
        "### **Objetivo da Q-Table**\n",
        "O objetivo do treinamento com Q-Learning é atualizar a Q-Table para que:\n",
        "1. O agente possa determinar a melhor ação a ser tomada em qualquer estado.\n",
        "2. A política ótima ($\\pi^*$) seja derivada diretamente da Q-Table. A política é obtida selecionando a ação com o maior valor Q em cada estado:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\arg\\max_{a} Q(s, a)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Atualização da Q-Table**\n",
        "Os valores Q na tabela são atualizados iterativamente durante o treinamento. A fórmula de atualização é:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "Explicando a equação:\n",
        "\n",
        "Novo Valor Q = Valor Q antigo + taxa de aprendizado * (recompensa + fator de desconto * valor Q máximo do próximo estado - Valor Q antigo)\n",
        "\n",
        "\n",
        "- $s$: Estado atual.\n",
        "- $a$: Ação tomada.\n",
        "- $r$: Recompensa recebida após executar a ação $a$ em $s$.\n",
        "- $s'$: Próximo estado alcançado.\n",
        "- $\\alpha$: Taxa de aprendizado (determina o peso dado à nova informação).\n",
        "- $\\gamma$: Fator de desconto (determina a importância das recompensas futuras).\n",
        "- $\\max_{a'} Q(s', a')$: Melhor valor Q estimado no próximo estado $s'$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretação dos Valores Q**\n",
        "- **Valores Altos:** Indicam que a ação em um estado específico leva a recompensas mais altas no futuro.\n",
        "- **Valores Baixos ou Negativos:** Indicam que a ação é menos vantajosa ou leva a penalidades.\n",
        "- **Zeros (Inicialmente):** Representam incerteza antes do treinamento.\n",
        "\n",
        "---\n",
        "\n",
        "### **Exemplo**\n",
        "Suponha um ambiente de navegação com um robô que precisa alcançar um objetivo, evitando obstáculos. Inicialmente, a Q-Table pode parecer algo assim:\n",
        "\n",
        "| Estado  | Up   | Down | Left | Right |\n",
        "|---------|-------|-------|-------|-------|\n",
        "| (0, 0)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (0, 1)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (0, 2)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (0, 3)  | 1.00 | 1.00 | 1.00 | 1.00  |\n",
        "\n",
        "Após o treinamento, a tabela pode ser atualizada para refletir os valores Q aprendidos:\n",
        "\n",
        "| Estado  | Up    | Down   | Left  | Right |\n",
        "|---------|-------|--------|-------|-------|\n",
        "| (0, 0)  | -0.1  | 0.50   | 0.00  | 0.75  |\n",
        "| (0, 1)  | 0.30  | 0.40   | 0.25  | 0.90  |\n",
        "| (0, 2)  | 0.60  | -0.20  | 0.50  | 1.00  |\n",
        "| (0, 3)  | 1.00  | 1.00   | 1.00  | 1.00  |\n",
        "\n",
        "Aqui:\n",
        "- O robô sabe que em $(0, 3)$, qualquer ação é ótima (valor $Q = 1.00$).\n",
        "- Em $(0, 2)$, a melhor ação é \"Right\" ($Q = 1.00$).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RpgFm5f7LByV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exemplo Detalhado de Preenchimento da Q-Table: Navegação de Robô**\n",
        "\n",
        "Vamos detalhar como a Q-Table é preenchida durante o treinamento para um problema de navegação em que um robô precisa alcançar um objetivo enquanto evita obstáculos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Descrição do Problema**\n",
        "\n",
        "- **Grid (3x4):**  \n",
        "  O ambiente é uma grade $3 \\times 4$, onde:\n",
        "  - $S = (0,0)$ é o estado inicial.\n",
        "  - $G = (2,3)$ é o objetivo.\n",
        "  - Obstáculos estão em $(1,1)$.\n",
        "  \n",
        "  O robô pode se mover nas direções: `\"up\"`, `\"down\"`, `\"left\"`, `\"right\"`. Se tentar mover para fora dos limites ou colidir com obstáculos, permanece no estado atual.\n",
        "\n",
        "- **Recompensas:**\n",
        "  - $+10$ ao alcançar o objetivo ($G$).\n",
        "  - $-1$ para cada movimento para incentivar soluções rápidas.\n",
        "  - $-10$ ao colidir com um obstáculo.\n",
        "\n",
        "- **Parâmetros de Treinamento:**\n",
        "  - $\\alpha = 0.1$ (taxa de aprendizado).\n",
        "  - $\\gamma = 0.9$ (fator de desconto).\n",
        "  - Inicialmente, todos os valores Q são $0$.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Etapas de Atualização**\n",
        "\n",
        "1. **Inicializar a Q-Table:**\n",
        "   Começamos com uma Q-Table preenchida com zeros:\n",
        "\n",
        "| Estado  | Up   | Down | Left | Right |\n",
        "|---------|------|------|------|-------|\n",
        "| (0, 0)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (0, 1)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (0, 2)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (0, 3)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (1, 0)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (1, 2)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (1, 3)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (2, 0)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (2, 1)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (2, 2)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "| (2, 3)  | 0.00 | 0.00 | 0.00 | 0.00  |\n",
        "\n",
        "2. **Episódio 1 (Passos):**\n",
        "\n",
        "   **(a) Estado Inicial:** $(0,0)$  \n",
        "   Ação escolhida aleatoriamente: `\"right\"`.  \n",
        "   Próximo estado: $(0,1)$.  \n",
        "   Recompensa: $-1$.\n",
        "\n",
        "   Atualização do valor $Q((0,0), \\text{\"right\"})$:\n",
        "\n",
        "   $$\n",
        "   Q((0,0), \\text{\"right\"}) = 0 + 0.1 \\left[ -1 + 0.9 \\cdot \\max(0, 0, 0, 0) - 0 \\right]\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   Q((0,0), \\text{\"right\"}) = -0.1\n",
        "   $$\n",
        "\n",
        "   **(b) Estado Atual:** $(0,1)$  \n",
        "   Ação: `\"down\"`.  \n",
        "   Próximo estado: $(1,1)$ (obstáculo).  \n",
        "   Recompensa: $-10$.\n",
        "\n",
        "   Atualização do valor $Q((0,1), \\text{\"down\"})$:\n",
        "\n",
        "   $$\n",
        "   Q((0,1), \\text{\"down\"}) = 0 + 0.1 \\left[ -10 + 0.9 \\cdot \\max(0, 0, 0, 0) - 0 \\right]\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   Q((0,1), \\text{\"down\"}) = -1\n",
        "   $$\n",
        "\n",
        "3. **Após Múltiplos Episódios:**\n",
        "   Após várias explorações e atualizações, a Q-Table reflete os valores esperados:\n",
        "\n",
        "| Estado  | Up    | Down   | Left  | Right |\n",
        "|---------|-------|--------|-------|-------|\n",
        "| (0, 0)  | -0.5  | 0.00   | 0.00  | 1.25  |\n",
        "| (0, 1)  | -0.5  | -1.00  | -0.5  | 2.50  |\n",
        "| (0, 2)  | -0.5  | -0.20  | -0.5  | 5.00  |\n",
        "| (0, 3)  | 10.00 | 10.00  | 10.00 | 10.00 |\n",
        "| (1, 0)  | -0.5  | 0.00   | -0.5  | 1.25  |\n",
        "| (1, 2)  | -0.5  | -0.20  | -0.5  | 2.50  |\n",
        "| (1, 3)  | -0.5  | -0.20  | -0.5  | 5.00  |\n",
        "| (2, 0)  | -0.5  | 0.00   | -0.5  | 1.25  |\n",
        "| (2, 1)  | -0.5  | -0.20  | -0.5  | 2.50  |\n",
        "| (2, 2)  | -0.5  | -0.20  | -0.5  | 5.00  |\n",
        "| (2, 3)  | 10.00 | 10.00  | 10.00 | 10.00 |\n",
        "\n",
        "---\n",
        "\n",
        "#### **Interpretação Final**\n",
        "1. **Estado Objetivo ($2,3$):**\n",
        "   Todos os valores Q para as ações são $10.00$, indicando que qualquer ação ali não tem custo adicional.\n",
        "\n",
        "2. **Outros Estados:**\n",
        "   - Os valores positivos em direções apontam para o caminho em direção ao objetivo.\n",
        "   - Valores negativos indicam que o movimento leva a penalizações.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FLFwnpJ4R-Iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outro explemplo de Q-Table\n",
        "Imagine um rato em um labirinto. O objetivo do rato é encontrar o queijo.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1SugGt3tVIiOeANEyITPBtbjJjEccUb3Q' width=400>\n",
        "\n",
        " * Estados: Cada posição no labirinto é um estado.\n",
        " * Ações: O rato pode se mover para cima, para baixo, para a esquerda ou para a direita.\n",
        " * Recompensas: O rato recebe uma grande recompensa ao encontrar o queijo e pequenas recompensas negativas ao se chocar com paredes.\n",
        "\n",
        "Tabela Q Inicial:\n",
        "\n",
        "| Estado | Acima | Abaixo | Esquerda | Direita |\n",
        "|---|---|---|---|---|\n",
        "| S1 | 0 | 0 | 0 | 0 |\n",
        "| S2 | 0 | 0 | 0 | 0 |\n",
        "| ... | ... | ... | ... | ... |\n",
        "\n",
        "Como funciona:\n",
        " * Inicialização: A tabela Q começa com todos os valores iguais a zero, indicando que o rato não tem conhecimento sobre o labirinto.\n",
        " * Exploração: O rato explora o labirinto aleatoriamente, tentando diferentes ações em cada estado.\n",
        " * Atualização da Tabela Q: A cada passo, o valor Q da ação escolhida é atualizado usando a seguinte fórmula (simplificada):\n",
        "\n",
        "   Novo Valor Q = Valor Q antigo + taxa de aprendizado * (recompensa + valor Q máximo do próximo estado - Valor Q antigo)\n",
        "\n",
        " * Exploration vs. Exploitation: Com o tempo, o rato começa a explorar menos aleatoriamente e a escolher as ações com os maiores valores Q, aumentando suas chances de encontrar o queijo mais rapidamente.\n",
        "\n",
        "Exemplo de Atualização:\n",
        "Suponha que o rato esteja no estado S1 e decida se mover para a direita. Ele encontra uma parede e recebe uma recompensa negativa. A tabela Q é atualizada para diminuir o valor Q da ação \"direita\" no estado S1, indicando que essa não é uma boa opção.\n",
        "\n",
        "Tabela Q após algumas iterações:\n",
        "\n",
        "| Estado | Acima | Abaixo | Esquerda | Direita |\n",
        "|---|---|---|---|---|\n",
        "| S1 | 0.1 | -0.2 | 0.3 | -0.5 |\n",
        "| S2 | ... | ... | ... | ... |\n",
        "\n",
        "Por que isso funciona:\n",
        "O Q-learning permite que o rato aprenda gradualmente qual a melhor ação a tomar em cada estado, maximizando a recompensa total ao longo do tempo. Essa técnica pode ser aplicada a uma variedade de problemas, desde jogos até robótica."
      ],
      "metadata": {
        "id": "Tbfctk_hsxfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q-Learning Metrics\n",
        "As métricas são essenciais para avaliar o desempenho do agente no treinamento de Q-Learning. Elas ajudam a entender se o agente está aprendendo e se está se aproximando da política ótima. Aqui estão as principais métricas utilizadas no contexto do Q-Learning:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Recompensa Acumulada por Episódio**\n",
        "- **Descrição:** Soma das recompensas recebidas pelo agente durante um único episódio.\n",
        "- **Propósito:** Avaliar a eficácia do agente em alcançar estados de alta recompensa.\n",
        "- **Interpretação:**\n",
        "  - Recompensa acumulada crescente ao longo do treinamento indica que o agente está aprendendo.\n",
        "  - Convergência da recompensa acumulada sugere que o agente encontrou uma política estável.\n",
        "- **Exemplo de Gráfico:** Plotar a recompensa acumulada por episódio ao longo do tempo.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(rewards_per_episode)\n",
        "plt.title(\"Recompensa Acumulada por Episódio\")\n",
        "plt.xlabel(\"Episódios\")\n",
        "plt.ylabel(\"Recompensa Acumulada\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Taxa de Sucesso**\n",
        "- **Descrição:** A proporção de episódios em que o agente alcançou o objetivo.\n",
        "- **Propósito:** Medir a eficácia do agente em alcançar o estado final desejado.\n",
        "- **Interpretação:**\n",
        "  - Uma taxa de sucesso próxima de 100% indica que o agente aprendeu a política ótima.\n",
        "- **Fórmula:**\n",
        "  $$\n",
        "  \\text{Taxa de Sucesso} = \\frac{\\text{Episódios com Sucesso}}{\\text{Total de Episódios}}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Número de Passos por Episódio**\n",
        "- **Descrição:** Número de passos (ações) realizados pelo agente em um episódio.\n",
        "- **Propósito:** Verificar a eficiência do agente em encontrar a solução.\n",
        "- **Interpretação:**\n",
        "  - Um número decrescente de passos por episódio indica que o agente está encontrando rotas mais curtas ou eficientes.\n",
        "- **Exemplo de Gráfico:**\n",
        "\n",
        "```python\n",
        "plt.plot(steps_per_episode)\n",
        "plt.title(\"Número de Passos por Episódio\")\n",
        "plt.xlabel(\"Episódios\")\n",
        "plt.ylabel(\"Passos\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Diferença de Valores Q**\n",
        "- **Descrição:** Média da diferença entre os valores Q atualizados e os anteriores (\\(\\Delta Q\\)).\n",
        "- **Propósito:** Avaliar a convergência da Q-Table.\n",
        "- **Interpretação:**\n",
        "  - Diferenças pequenas sugerem que o agente está convergindo para uma solução estável.\n",
        "- **Fórmula:**\n",
        "  $$\n",
        "  \\Delta Q = \\frac{\\sum |Q_{\\text{novo}} - Q_{\\text{anterior}}|}{\\text{Total de Estados e Ações}}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Exploration vs. Exploitation (Razão de Ações Exploratórias)**\n",
        "- **Descrição:** A proporção de ações exploratórias (escolhidas aleatoriamente) em comparação às ações exploitation (com base na política atual).\n",
        "- **Propósito:** Avaliar o equilíbrio entre exploration e exploitation.\n",
        "- **Interpretação:**\n",
        "  - Durante o início do treinamento, mais ações exploratórias são esperadas.\n",
        "  - Com o tempo, a exploração deve diminuir, e a exploitation aumentar.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Valor Médio da Q-Table**\n",
        "- **Descrição:** Média de todos os valores Q na tabela.\n",
        "- **Propósito:** Monitorar a evolução geral da estimativa de recompensa futura.\n",
        "- **Interpretação:**\n",
        "  - Valores Q crescentes indicam que o agente está aprendendo a maximizar a recompensa.\n",
        "- **Fórmula:**\n",
        "  $$\n",
        "  Q_{\\text{médio}} = \\frac{\\sum Q(s, a)}{\\text{Total de Estados e Ações}}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Desempenho em Ambientes Não Explorados**\n",
        "- **Descrição:** Avaliar o desempenho do agente em novos ambientes ou estados que não foram explorados durante o treinamento.\n",
        "- **Propósito:** Testar a capacidade de generalização do agente.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Estabilidade da Política**\n",
        "- **Descrição:** Frequência com que a política (melhor ação para cada estado) muda durante o treinamento.\n",
        "- **Propósito:** Medir a convergência da política.\n",
        "- **Interpretação:**\n",
        "  - Uma política estável sugere que o agente convergiu para uma solução ótima.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Tempo de Treinamento**\n",
        "- **Descrição:** O tempo total necessário para treinar o agente até a convergência.\n",
        "- **Propósito:** Avaliar a eficiência computacional do algoritmo.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Consistência dos Resultados**\n",
        "- **Descrição:** Variância na recompensa acumulada, taxa de sucesso ou passos por episódio em execuções diferentes.\n",
        "- **Propósito:** Avaliar a robustez do aprendizado.\n",
        "- **Interpretação:**\n",
        "  - Baixa variância indica que o agente é consistente.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumo das Métricas**\n",
        "| **Métrica**               | **Propósito**                                  | **Exemplo de Uso**                 |\n",
        "|---------------------------|-----------------------------------------------|-----------------------------------|\n",
        "| Recompensa Acumulada      | Avaliar a eficácia do aprendizado.             | Crescimento ao longo do tempo.   |\n",
        "| Taxa de Sucesso           | Avaliar se o agente alcança o objetivo.        | Sucesso em 90% dos episódios.    |\n",
        "| Número de Passos          | Medir a eficiência do agente.                  | Caminho mais curto.              |\n",
        "| Diferença de Valores Q    | Monitorar a convergência da Q-Table.           | Diferença tende a 0.             |\n",
        "| Exploração vs. Exploração | Verificar equilíbrio durante o aprendizado.     | Razão de exploração diminui.     |\n",
        "| Valor Médio da Q-Table    | Avaliar a evolução geral da estimativa.         | Valores crescentes.              |\n",
        "| Estabilidade da Política  | Monitorar mudanças frequentes na política.     | Política converge.               |\n",
        "\n",
        "---\n",
        "\n",
        "### Exemplo de Aplicação no Python\n",
        "\n",
        "```python\n",
        "# Monitorar métricas durante o treinamento\n",
        "rewards_per_episode = []\n",
        "steps_per_episode = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    state = start_state\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    \n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward = take_action(state, action)\n",
        "        \n",
        "        # Atualizar Q-Table\n",
        "        q_table[state, action] = update_q_value(state, action, reward, next_state)\n",
        "        \n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        \n",
        "        if state == goal_state:\n",
        "            done = True\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    steps_per_episode.append(steps)\n",
        "```"
      ],
      "metadata": {
        "id": "1-zo1-mehIDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Referências\n",
        "\n",
        "Artigos\n",
        "\n",
        "1. \"Q-learning\" - Watkins e Dayan (1992)\n",
        "2. \"Learning from Delayed Rewards\" - Watkins (1989)\n",
        "3. \"Deep Q-Networks\" - Mnih et al. (2015)\n",
        "4. \"Double Q-Learning\" - Hasselt (2010)\n",
        "\n",
        "Livros\n",
        "1. \"Reinforcement Learning: An Introduction\" - Richard S. Sutton e Andrew G. Barto (2018)\n",
        "   - Autores: Richard S. Sutton e Andrew G. Barto  \n",
        "   - Descrição: Considerado o livro clássico sobre aprendizado por reforço, inclui capítulos detalhados sobre Q-Learning e outros métodos.\n",
        "   - [Link oficial](http://incompleteideas.net/book/the-book.html)\n",
        "\n",
        "2. \"Deep Reinforcement Learning\" - Sutton e Barto (2019)\n",
        "3. \"Q-Learning e Aprendizado de Máquina\" - Luciano Vargas (2019)\n",
        "4. \"Inteligência Artificial: Uma Abordagem Moderna\" - Stuart Russell e Peter Norvig (2014)\n",
        "  - [Artificial Intelligence: A Modern Approach (em inglês)](https://www.amazon.com.br/Artificial-Intelligence-Approach-Stuart-Russell/dp/0134610997)\n",
        "\n",
        "  - [Inteligência Artificial - Uma Abordagem Moderna (versão em português)](https://www.amazon.com.br/Intelig%C3%AAncia-Artificial-Uma-Abordagem-Moderna/dp/8595158878)\n",
        "\n",
        "Artigos Científicos\n",
        "1. **Learning from Delayed Rewards**  \n",
        "   - Autor: Chris J.C.H. Watkins (1992)  \n",
        "   - Descrição: Este é o artigo seminal que introduziu o Q-Learning.\n",
        "   - [Link para o artigo](https://link.springer.com/article/10.1007/BF00992698)\n",
        "\n",
        "2. **Playing Atari with Deep Reinforcement Learning**  \n",
        "   - Autores: Mnih et al. (2013)  \n",
        "   - Descrição: Este artigo conecta Q-Learning com redes neurais profundas (Deep Q-Networks).\n",
        "   - [Link para o artigo](https://arxiv.org/abs/1312.5602)\n",
        "\n",
        "1. \"Q-learning\" - Watkins e Dayan (1992)\n",
        "2. \"Learning from Delayed Rewards\" - Watkins (1989)\n",
        "3. \"Deep Q-Networks\" - Mnih et al. (2015)\n",
        "4. \"Double Q-Learning\" - Hasselt (2010)\n",
        "\n",
        "Tutoriais Online e Cursos\n",
        "\n",
        "1. **OpenAI Spinning Up**  \n",
        "   - Descrição: Fornece uma introdução prática ao aprendizado por reforço, incluindo Q-Learning.\n",
        "   - [Link para o site](https://spinningup.openai.com)\n",
        "\n",
        "2. **Coursera - Reinforcement Learning Specialization**  \n",
        "   - Instituição: University of Alberta  \n",
        "   - Descrição: Curso online que inclui tópicos de Q-Learning.\n",
        "   - [Acesse aqui](https://www.coursera.org/specializations/reinforcement-learning)\n",
        "\n",
        "3. **Stanford CS234: Reinforcement Learning**  \n",
        "   - Descrição: Slides e vídeos que cobrem Q-Learning e outros tópicos relacionados.\n",
        "   - [Site do curso](http://web.stanford.edu/class/cs234/)\n",
        "\n",
        "Repositórios no GitHub\n",
        "\n",
        "1. **Q-Learning Implementation**  \n",
        "   - Autor: PacktPublishing  \n",
        "   - [Repositório](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-with-Python/tree/master/Chapter02)\n",
        "\n",
        "2. **OpenAI Gym Examples**  \n",
        "   - Descrição: Exemplos de implementações práticas de aprendizado por reforço com ambientes do OpenAI Gym.\n",
        "   - [Repositório](https://github.com/openai/gym)\n",
        "\n",
        "Blogs e Artigos Populares\n",
        "\n",
        "1. **Simple Reinforcement Learning with TensorFlow**  \n",
        "   - Blog: Medium (Morvan Zhou)  \n",
        "   - Descrição: Explicação passo a passo do Q-Learning.\n",
        "   - [Leia aqui](https://morvanzhou.github.io/tutorials/)\n",
        "\n",
        "2. **Understanding Q-Learning**  \n",
        "   - Blog: Towards Data Science  \n",
        "   - [Leia aqui](https://towardsdatascience.com)\n",
        "\n"
      ],
      "metadata": {
        "id": "lmgFlBWeCFlf"
      }
    }
  ]
}