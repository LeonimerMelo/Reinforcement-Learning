{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6mDqWPCTrEL/nkIWrQisU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonimerMelo/Reinforcement-Learning/blob/Reinforcement-Learning/Multi_Armed_Bandit_Problem_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandit Problem\n",
        "O **Problema do Bandido Multibraço (Multi-Armed Bandit)** é um problema fundamental em **Aprendizado por Reforço (Reinforcement Learning)**, frequentemente usado para estudar o equilíbrio entre **exploração** e **exploração**. Ele envolve um agente que toma decisões (puxar as alavancas de um bandido) para maximizar a recompensa total ao longo do tempo.\n",
        "\n",
        "---\n",
        "\n",
        "### Configuração do Problema\n",
        "\n",
        "1. **Cenário**:\n",
        "   - Imagine uma máquina caça-níqueis (um \"bandido\") com $ n $ alavancas.\n",
        "   - Cada alavanca $ i $ fornece uma recompensa retirada de uma distribuição de probabilidade desconhecida com recompensa esperada $ \\mu_i $.\n",
        "\n",
        "2. **Objetivo**:\n",
        "   - Identificar a melhor alavanca (ou um conjunto de boas alavancas) que maximiza a recompensa esperada ao longo de $ T $ passos de tempo.\n",
        "\n",
        "3. **Dilema (Trade-Off)**:\n",
        "   - **Exploration**: Experimentar alavancas para coletar mais informações sobre suas recompensas.\n",
        "   - **Exploitation**: Escolher a alavanca que, com base nas informações anteriores, parece ser a melhor.\n",
        "\n",
        "---\n",
        "\n",
        "### Algoritmos para Resolver o Problema\n",
        "\n",
        "Diversos algoritmos foram desenvolvidos para lidar com o dilema exploração-exploração:\n",
        "\n",
        "#### 1. **Algoritmo ε-Greedy**\n",
        "   - Com probabilidade $ \\epsilon $, explora (escolhe uma alavanca aleatória).\n",
        "   - Com probabilidade $ 1 - \\epsilon $, explora (escolhe a alavanca com a maior recompensa estimada).\n",
        "\n",
        "  Pseudocódigo:\n",
        "  ```python\n",
        "  if random() < epsilon:\n",
        "      action = random_arm()\n",
        "  else:\n",
        "      action = arm_with_highest_estimated_reward()\n",
        "   ```\n",
        "\n",
        "#### 2. **UCB (Upper Confidence Bound)**\n",
        "   - Equilibra exploração e exploração usando um intervalo de confiança para cada alavanca.\n",
        "   - Escolhe a alavanca com o maior limite superior:\n",
        "     $$\n",
        "     a_t = \\arg\\max_i \\left( \\hat{\\mu}_i + \\sqrt{\\frac{2 \\ln t}{n_i}} \\right)\n",
        "     $$\n",
        "     Onde:\n",
        "     - $ \\hat{\\mu}_i $: Recompensa estimada da alavanca $ i $.\n",
        "     - $ n_i $: Número de vezes que a alavanca $ i $ foi escolhida.\n",
        "     - $ t $: Passo de tempo atual.\n",
        "\n",
        "#### 3. **Thompson Sampling**\n",
        "   - Uma abordagem Bayesiana para equilibrar exploração e exploração.\n",
        "   - Modela a recompensa de cada alavanca como uma distribuição de probabilidade e escolhe ações com base em amostras dessas distribuições.\n",
        "\n",
        "#### 4. **Seleção Softmax**\n",
        "   - Usa uma abordagem probabilística para escolher alavancas com base em suas recompensas estimadas.\n",
        "   - A probabilidade de escolher a alavanca $ i $ é proporcional a:\n",
        "$$P(i) = \\frac{e^{\\hat{\\mu}_i / \\tau}}{\\sum_{j}^{e^{\\hat{\\mu}_i / \\tau}}}$$\n",
        "Onde $\\tau$ (temperatura) controla o equilíbrio entre *exploration-exploitation trade-off*.\n",
        "\n",
        "---\n",
        "\n",
        "### Exemplo em Python: Algoritmo ε-Greedy\n",
        "\n",
        "Aqui está uma implementação básica do algoritmo **ε-Greedy**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Configuração\n",
        "num_arms = 5\n",
        "num_steps = 1000\n",
        "true_rewards = np.random.rand(num_arms)  # Recompensas verdadeiras para cada alavanca\n",
        "epsilon = 0.1  # Taxa de exploração\n",
        "\n",
        "# Inicialização\n",
        "estimated_rewards = np.zeros(num_arms)  # Recompensas estimadas\n",
        "counts = np.zeros(num_arms)  # Número de vezes que cada alavanca foi escolhida\n",
        "total_reward = 0\n",
        "\n",
        "# Simulação\n",
        "for t in range(num_steps):\n",
        "    if np.random.rand() < epsilon:\n",
        "        # Explorar: Escolher uma alavanca aleatória\n",
        "        arm = np.random.randint(num_arms)\n",
        "    else:\n",
        "        # Explorar: Escolher a alavanca com a maior recompensa estimada\n",
        "        arm = np.argmax(estimated_rewards)\n",
        "    \n",
        "    # Puxar a alavanca escolhida\n",
        "    reward = np.random.rand() < true_rewards[arm]  # Recompensa binária simulada (0 ou 1)\n",
        "    \n",
        "    # Atualizar estimativas\n",
        "    counts[arm] += 1\n",
        "    estimated_rewards[arm] += (reward - estimated_rewards[arm]) / counts[arm]\n",
        "    \n",
        "    # Atualizar recompensa total\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Recompensa Total: {total_reward}\")\n",
        "print(f\"Recompensas Verdadeiras: {true_rewards}\")\n",
        "print(f\"Recompensas Estimadas: {estimated_rewards}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Métricas-Chave\n",
        "\n",
        "1. **Regret (Arrependimento)**:\n",
        "   - Mede a perda causada por não escolher sempre a melhor alavanca.\n",
        "   - $ R(T) = T \\cdot \\mu^* - \\sum_{t=1}^T \\mu_{a_t} $\n",
        "     - $ \\mu^* $: Recompensa esperada da melhor alavanca.\n",
        "     - $ \\mu_{a_t} $: Recompensa esperada da alavanca escolhida no tempo $ t $.\n",
        "\n",
        "2. **Recompensa Cumulativa**:\n",
        "   - Recompensa total obtida ao longo de $ T $ passos de tempo.\n",
        "\n",
        "---\n",
        "\n",
        "### Aplicações\n",
        "\n",
        "1. **Sistemas de Recomendação**:\n",
        "   - Sugerir itens com base nas preferências do usuário (e.g., filmes, produtos).\n",
        "\n",
        "2. **Ensaios Clínicos**:\n",
        "   - Testar diferentes tratamentos enquanto maximiza os resultados dos pacientes.\n",
        "\n",
        "3. **Publicidade Online**:\n",
        "   - Escolher anúncios que maximizem a taxa de cliques.\n",
        "\n",
        "4. **IA para Jogos**:\n",
        "   - Equilibrar exploração e exploração em tomadas de decisão estratégicas.\n",
        "\n",
        "O **Problema do Bandido Multibraço** é um modelo simples, mas poderoso, que serve como ponto de partida para entender problemas mais complexos no aprendizado por reforço!"
      ],
      "metadata": {
        "id": "yOgAxV2WYLJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Setup\n",
        "num_arms = 5\n",
        "num_steps = 1000\n",
        "np.random.seed(10)\n",
        "true_rewards = np.random.rand(num_arms)  # True reward probabilities for each arm\n",
        "epsilon = 0.2  # Exploration rate\n",
        "\n",
        "# Initialization\n",
        "estimated_rewards = np.zeros(num_arms)  # Estimated rewards for each arm\n",
        "counts = np.zeros(num_arms)  # Number of times each arm was pulled\n",
        "total_reward = 0\n",
        "\n",
        "# Simulation\n",
        "for t in range(num_steps):\n",
        "    if np.random.rand() < epsilon:\n",
        "        # Explore: Choose a random arm\n",
        "        arm = np.random.randint(num_arms)\n",
        "    else:\n",
        "        # Exploit: Choose the arm with the highest estimated reward\n",
        "        arm = np.argmax(estimated_rewards)\n",
        "\n",
        "    # Pull the chosen arm\n",
        "    reward = np.random.rand() < true_rewards[arm]  # Simulated binary reward (0 or 1)\n",
        "\n",
        "    # Update estimates\n",
        "    counts[arm] += 1\n",
        "    estimated_rewards[arm] += (reward - estimated_rewards[arm]) / counts[arm]\n",
        "\n",
        "    # Update total reward\n",
        "    total_reward += reward\n",
        "\n",
        "# print(f\"Total Reward: {total_reward}\")\n",
        "# print(f\"True Rewards: {true_rewards}\")\n",
        "# print(f\"Estimated Rewards: {estimated_rewards}\")\n",
        "\n",
        "# Resultados\n",
        "print(\"Probabilidades Reais: \", true_rewards)\n",
        "print(\"Recompensas Estimadas: \", estimated_rewards)\n",
        "print(\"Seleções por Alavanca: \", counts)\n",
        "print(\"Recompensa Cumulativa Total: \", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axNZPihqcRtH",
        "outputId": "718cfca8-25b4-4a4a-ae79-6290eca83211"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades Reais:  [0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n",
            "Recompensas Estimadas:  [0.78494624 0.06666667 0.72340426 0.77142857 0.53846154]\n",
            "Seleções por Alavanca:  [744.  30.  47. 140.  39.]\n",
            "Recompensa Cumulativa Total:  749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atualizar a estimativa\n",
        "\n",
        "```python\n",
        "# Update estimates\n",
        "counts[arm] += 1\n",
        "estimated_rewards[arm] += (reward - estimated_rewards[arm]) / counts[arm]\n",
        "\n",
        "```\n",
        "Esta parte do código é responsável por **atualizar a estimativa de recompensa média** para a alavanca escolhida com base no novo resultado obtido (recompensa). Vamos detalhar cada linha:\n",
        "\n",
        "---\n",
        "\n",
        "### Linha 1: `counts[arm] += 1`\n",
        "\n",
        "- **O que faz:** Incrementa o contador de quantas vezes a alavanca $ arm $ foi puxada.\n",
        "- **Por que é necessário:**\n",
        "  - Para calcular a média incremental, precisamos saber o número total de vezes que a alavanca foi escolhida.\n",
        "  - Esse valor será usado no cálculo da nova média.\n",
        "\n",
        "Exemplo:\n",
        "- Se a alavanca $ 2 $ foi puxada 3 vezes anteriormente, após esta linha, o contador será atualizado para 4.\n",
        "\n",
        "---\n",
        "\n",
        "### Linha 2: `estimated_rewards[arm] += (reward - estimated_rewards[arm]) / counts[arm]`\n",
        "\n",
        "Essa linha usa a **fórmula de atualização incremental da média** para ajustar a recompensa estimada ($ \\hat{\\mu}_i $) da alavanca $ arm $ com base na nova recompensa $ reward $.\n",
        "\n",
        "#### **Fórmula de Atualização Incremental**\n",
        "A fórmula básica da média é:\n",
        "$$\n",
        "\\hat{\\mu}_i = \\frac{\\text{soma dos valores observados}}{\\text{número de observações}}\n",
        "$$\n",
        "No entanto, recalcular a soma de todos os valores observados a cada atualização é ineficiente. Em vez disso, usamos uma fórmula incremental para atualizar a média sem precisar armazenar todas as recompensas anteriores:\n",
        "\n",
        "$$\n",
        "\\hat{\\mu}_i \\gets \\hat{\\mu}_i + \\frac{\\text{(nova recompensa - média atual)}}{\\text{número de vezes que a alavanca foi puxada}}\n",
        "$$\n",
        "\n",
        "- **Parte 1:** $(\\text{reward} - \\text{estimated_rewards[arm]})$\n",
        "  - Calcula a diferença entre a recompensa recém-obtida ($ reward $) e a recompensa média atual estimada ($ \\text{estimated_rewards[arm]} $).\n",
        "  - Essa diferença indica o quanto o novo valor difere da média atual.\n",
        "\n",
        "- **Parte 2:** $(\\text{reward} - \\text{estimated_rewards[arm]}) / \\text{counts[arm]}$\n",
        "  - Ajusta a diferença pelo número total de vezes que a alavanca foi puxada.\n",
        "  - Isso garante que a média seja alterada de forma gradual e proporcional à quantidade de dados já observados.\n",
        "\n",
        "- **Parte 3:** `estimated_rewards[arm] += ...`\n",
        "  - Atualiza a recompensa média incrementalmente, incorporando a nova observação.\n",
        "\n",
        "#### Exemplo Prático:\n",
        "Imagine que:\n",
        "- A recompensa estimada atual para a alavanca $ 2 $ seja $ \\hat{\\mu}_2 = 0.5 $,\n",
        "- Essa alavanca foi puxada 4 vezes,\n",
        "- A nova recompensa obtida ($ reward $) seja $ 1.0 $.\n",
        "\n",
        "1. **Atualizar o contador**:\n",
        "   $$\n",
        "   \\text{counts[2]} \\gets 5\n",
        "   $$\n",
        "\n",
        "2. **Calcular a diferença entre a nova recompensa e a média atual**:\n",
        "   $$\n",
        "   \\text{(reward - estimated_rewards[2])} = 1.0 - 0.5 = 0.5\n",
        "   $$\n",
        "\n",
        "3. **Atualizar a média**:\n",
        "   $$\n",
        "   \\text{estimated_rewards[2]} \\gets 0.5 + \\frac{0.5}{5} = 0.5 + 0.1 = 0.6\n",
        "   $$\n",
        "\n",
        "Agora, a recompensa média estimada para a alavanca $ 2 $ foi ajustada para $ 0.6 $, levando em consideração a nova observação.\n",
        "\n",
        "---\n",
        "\n",
        "### Por que usar a fórmula incremental?\n",
        "\n",
        "1. **Eficiência Computacional**:\n",
        "   - Não precisamos armazenar todas as recompensas passadas para recalcular a média.\n",
        "   - Apenas o valor atual da média e o número de observações são suficientes.\n",
        "\n",
        "2. **Atualização Dinâmica**:\n",
        "   - Cada nova recompensa tem menos impacto na média à medida que o número de observações aumenta, o que reflete melhor a confiança na estimativa.\n",
        "\n",
        "---\n",
        "\n",
        "### Resumo\n",
        "Essa fórmula de atualização é uma maneira elegante e eficiente de calcular a média incrementalmente, permitindo que o algoritmo ajuste continuamente sua estimativa de recompensa com base em novos dados. É especialmente útil em problemas como o bandido multibraço, onde as recompensas são observadas progressivamente."
      ],
      "metadata": {
        "id": "_9YuyHol9vjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimated_rewards = np.zeros(num_arms)  # Estimated rewards for each arm\n",
        "counts = np.zeros(num_arms)  # Number of times each arm was pulled\n",
        "total_reward = 0\n",
        "print(estimated_rewards)\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vwxPDeA5qzj",
        "outputId": "b1feae9a-63cf-4122-9969-b2c4e62629e6"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_rewards = np.random.rand(num_arms)\n",
        "true_rewards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfIddsyP6dxB",
        "outputId": "85d924ac-6ac0-4450-fee4-e5653ca548e3"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.41762441, 0.7373844 , 0.91337874, 0.34114791, 0.55892045])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvnMI2C956ny",
        "outputId": "b855281b-15b1-406c-a9c3-7ab07cd85689"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Começa o loop:"
      ],
      "metadata": {
        "id": "6t8VHpNa6rlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.random.rand()\n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFugniQZ5zFJ",
        "outputId": "6088e553-4453-4254-c973-627ab48f71a4"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1811683077412788"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if c < epsilon:\n",
        "    # Explore: Choose a random arm\n",
        "    arm = np.random.randint(num_arms)\n",
        "    print('exploration')\n",
        "else:\n",
        "    # Exploit: Choose the arm with the highest estimated reward\n",
        "    arm = np.argmax(estimated_rewards)\n",
        "    print('exploitation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRLWKQ765udx",
        "outputId": "156318b4-ff9f-4719-ba10-a1ad5d4b4815"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exploration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB6SQYqC24za",
        "outputId": "51fe68fa-9ced-4edb-d001-b99751988d8f"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_rewards[arm]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ9QCApd20Ii",
        "outputId": "08a4e484-9d60-4480-efd5-7c9460f4548c"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34114790713481635"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = np.random.rand()\n",
        "r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTRHQnOM3Gb6",
        "outputId": "c70b43ef-c1d5-48ec-9da9-36bb5763fc2d"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17470270334649896"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the chosen arm and receve the reward (or not!)\n",
        "reward = r < true_rewards[arm]\n",
        "# reward is binary: True (=1) or False (=0)\n",
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu2Ix7XL3Nvs",
        "outputId": "94f7bdd5-96d1-476d-f5d6-a3ad7960865e"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts[arm] += 1\n",
        "counts[arm]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k_KAHej3prD",
        "outputId": "c734fe1c-4df8-41b3-8b0d-45fe100c3541"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIsl78Pz8q1r",
        "outputId": "d519cbe3-655a-430c-d124-285eeb1a6220"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 0., 4., 5., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimated_rewards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YET_mjvK8-3Q",
        "outputId": "4f4e0109-dec1-4400-a6b3-9a870ebc9720"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0. , 0. , 1. , 0.5, 0. ])"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = estimated_rewards[arm]\n",
        "e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MH58jkw4HlE",
        "outputId": "69742e5f-1b57-4ee1-ab3b-b162dc505240"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a diferença entre a recompensa recém-obtida (reward) e a recompensa média atual estimada\n",
        "a = reward - e\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvniCVPT30ia",
        "outputId": "f70a1a9f-b0a1-4e3d-caf3-0b30612265e7"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajusta a diferença pelo número total de vezes que a alavanca foi puxada\n",
        "b = a / counts[arm]\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6riAc7c4456",
        "outputId": "03ce5706-5957-4e42-8d39-3ac4dd3e83ad"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Atualiza a recompensa média incrementalmente\n",
        "estimated_rewards[arm] = estimated_rewards[arm] + b\n",
        "estimated_rewards[arm]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFK-C28J5HFC",
        "outputId": "7bb5c733-cfd9-471f-adac-abeebcb1af9b"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_reward += reward\n",
        "total_reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZy1B7AV5Tb7",
        "outputId": "537f994e-37a0-4827-edda-0dbc5a4a5cb8"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retorna ao loop"
      ],
      "metadata": {
        "id": "Ikubt1Fc7h7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de Recompensa Cumulativa no Problema do Bandido Multibraço\n",
        "\n",
        "A **recompensa cumulativa** é a soma das recompensas obtidas ao longo de vários passos de tempo. Vamos construir um exemplo prático baseado no algoritmo **ε-Greedy** para calcular a recompensa cumulativa ao final de uma simulação.\n",
        "\n",
        "---\n",
        "\n",
        "### Código: Calculando Recompensa Cumulativa\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Configuração\n",
        "num_arms = 3  # Número de alavancas\n",
        "num_steps = 100  # Número de passos\n",
        "true_rewards = [0.2, 0.5, 0.8]  # Probabilidades reais de recompensa para cada alavanca\n",
        "epsilon = 0.1  # Taxa de exploração\n",
        "\n",
        "# Inicialização\n",
        "estimated_rewards = np.zeros(num_arms)  # Recompensas estimadas\n",
        "counts = np.zeros(num_arms)  # Contador de seleções por alavanca\n",
        "total_reward = 0  # Recompensa cumulativa inicial\n",
        "\n",
        "# Simulação\n",
        "for step in range(num_steps):\n",
        "    # Escolher alavanca: Explorar ou Explorar\n",
        "    if np.random.rand() < epsilon:\n",
        "        arm = np.random.randint(num_arms)  # Escolher alavanca aleatória (explorar)\n",
        "    else:\n",
        "        arm = np.argmax(estimated_rewards)  # Escolher alavanca com maior recompensa estimada (explorar)\n",
        "    \n",
        "    # Puxar a alavanca escolhida e obter recompensa\n",
        "    reward = np.random.rand() < true_rewards[arm]  # Simula recompensa (0 ou 1)\n",
        "    \n",
        "    # Atualizar estimativas\n",
        "    counts[arm] += 1\n",
        "    estimated_rewards[arm] += (reward - estimated_rewards[arm]) / counts[arm]\n",
        "    \n",
        "    # Atualizar recompensa cumulativa\n",
        "    total_reward += reward\n",
        "\n",
        "# Resultados\n",
        "print(\"Probabilidades Reais: \", true_rewards)\n",
        "print(\"Recompensas Estimadas: \", estimated_rewards)\n",
        "print(\"Seleções por Alavanca: \", counts)\n",
        "print(\"Recompensa Cumulativa Total: \", total_reward)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Saída Exemplo (simulada)\n",
        "\n",
        "Após executar o código acima, você pode obter algo como:\n",
        "\n",
        "```plaintext\n",
        "Probabilidades Reais:  [0.2, 0.5, 0.8]\n",
        "Recompensas Estimadas:  [0.18, 0.52, 0.76]\n",
        "Seleções por Alavanca:  [10, 30, 60]\n",
        "Recompensa Cumulativa Total:  68\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Explicação dos Resultados\n",
        "\n",
        "1. **Recompensas Verdadeiras**:\n",
        "   - A probabilidade real de cada alavanca fornecer uma recompensa (exemplo: 20%, 50%, e 80%).\n",
        "\n",
        "2. **Recompensas Estimadas**:\n",
        "   - As estimativas calculadas pelo agente com base nas interações.\n",
        "\n",
        "3. **Seleções por Alavanca**:\n",
        "   - Quantas vezes cada alavanca foi puxada.\n",
        "\n",
        "4. **Recompensa Cumulativa Total**:\n",
        "   - A soma de todas as recompensas obtidas durante os $ num\\_steps $.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação da Recompensa Cumulativa\n",
        "\n",
        "- Quanto mais o algoritmo consegue explorar as alavancas certas (com maior probabilidade de recompensa), maior será a **recompensa cumulativa**.\n",
        "- No exemplo, a alavanca com 80% de probabilidade de recompensa foi puxada com mais frequência, resultando em uma alta recompensa total.\n",
        "\n",
        "Este exemplo ilustra como o conceito de **recompensa cumulativa** mede o sucesso do agente ao equilibrar exploração e exploração no problema do bandido multibraço!"
      ],
      "metadata": {
        "id": "KHdBTI3UgD8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Configuração\n",
        "num_arms = 3  # Número de alavancas\n",
        "num_steps = 100  # Número de passos\n",
        "true_rewards = [0.2, 0.5, 0.8]  # Probabilidades reais de recompensa para cada alavanca\n",
        "epsilon = 0.1  # Taxa de exploração\n",
        "\n",
        "# Inicialização\n",
        "estimated_rewards = np.zeros(num_arms)  # Recompensas estimadas\n",
        "counts = np.zeros(num_arms)  # Contador de seleções por alavanca\n",
        "total_reward = 0  # Recompensa cumulativa inicial\n",
        "\n",
        "# Simulação\n",
        "for step in range(num_steps):\n",
        "    # Escolher alavanca: Explorar ou Explorar\n",
        "    if np.random.rand() < epsilon:\n",
        "        arm = np.random.randint(num_arms)  # Escolher alavanca aleatória (explorar)\n",
        "    else:\n",
        "        arm = np.argmax(estimated_rewards)  # Escolher alavanca com maior recompensa estimada (explorar)\n",
        "\n",
        "    # Puxar a alavanca escolhida e obter recompensa\n",
        "    reward = np.random.rand() < true_rewards[arm]  # Simula recompensa (0 ou 1)\n",
        "\n",
        "    # Atualizar estimativas\n",
        "    counts[arm] += 1\n",
        "    estimated_rewards[arm] += (reward - estimated_rewards[arm]) / counts[arm]\n",
        "\n",
        "    # Atualizar recompensa cumulativa\n",
        "    total_reward += reward\n",
        "\n",
        "# Resultados\n",
        "print(\"Probabilidades Reais: \", true_rewards)\n",
        "print(\"Recompensas Estimadas: \", estimated_rewards)\n",
        "print(\"Seleções por Alavanca: \", counts)\n",
        "print(\"Recompensa Cumulativa Total: \", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wEn8deOgXf7",
        "outputId": "3996cb73-6ae1-4057-db2e-ade37b0b7be8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades Reais:  [0.2, 0.5, 0.8]\n",
            "Recompensas Estimadas:  [0.25  0.375 0.25 ]\n",
            "Seleções por Alavanca:  [88.  8.  4.]\n",
            "Recompensa Cumulativa Total:  26\n"
          ]
        }
      ]
    }
  ]
}