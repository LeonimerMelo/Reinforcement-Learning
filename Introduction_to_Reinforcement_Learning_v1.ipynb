{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4j20KBbrarRG+DMXgo1Z8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonimerMelo/Reinforcement-Learning/blob/Reinforcement-Learning/Introduction_to_Reinforcement_Learning_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Reinforcement Learning\n",
        "\n",
        "Even though, Deep Learning (DL) has a powerful data representation ability but it is not enough to build a smart AI system. This is because an AI system should not only able to learn\n",
        "from the provided data but also able to learn from interactions with the real world\n",
        "environment like a human. RL is a subset of  Machine Learning (ML) that enables computers to learn by\n",
        "interacting with the real world environment.\n",
        "In brief, Reinforcement Learning (RL) separates the real world into two components—an environment and\n",
        "an agent. The agent interacts with the environment by performing specific actions\n",
        "and receives feedback from the environment. The feedback is usually termed as\n",
        "the “reward” in RL. The agent learns to perform “better” by trying to get more\n",
        "positive rewards from the environment. This learning process forms a feedback loop\n",
        "between the environment and agent, guiding the improvement of the agent with RL\n",
        "algorithms.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=17PBc0l2gKPuU6wKFHuzRNl3sJfkdMuWs' width=400>\n",
        "</center>\n",
        "\n",
        "Relationship of\n",
        "artificial intelligent, machine\n",
        "learning, deep learning,\n",
        "reinforcement learning, and\n",
        "deep reinforcement learning.\n",
        "\n",
        "###Qual a diferença em relação a outros métodos de Machine Learning?\n",
        "\n",
        "<center>\n",
        "<img src=https://miro.medium.com/v2/resize:fit:750/format:webp/1*aBoIm7kwgbeDCbOg447U_w.jpeg width=600>\n",
        "</center>\n",
        "\n",
        "- **Aprendizado supervisionado**: É necessário um dataset de treino, no qual é sabido os valores corretos de saída para as entradas correspondentes. A partir dele, é treinado o algoritmo que aprende a predizer resultados, com certo grau de acurácia, previamente desconhecidos.\n",
        "- **Aprendizado não supervisionado**: o algoritmo procura separar os dados em grupos com características similares. Não há dataset para treino, ou seja, não há informação prévia sobre em qual grupo pertence uma determinada entrada.\n",
        "\n",
        "Comparando com os anteriores, Reinforcement Learning se assemelha com o primeiro no sentido de aprender com as experiências anteriores e com o segundo no sentido de aprender de uma forma não supervisionada, porém essas experiências geralmente são simuladas, como veremos a seguir, e o aprendizado se dá tanto com os acertos quanto com os erros."
      ],
      "metadata": {
        "id": "5Mlb--zY7Gwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reinforcement Learning\n",
        "A ideia básica é que um agente procura cumprir uma determinada tarefa, inicialmente com um abordagem de tentativa e erro. Posteriormente, os resultados de cada tentativa, independente de seu sucesso, são utilizados para treinar o agente por meio de um sistema de recompensa/punição e determinar se as ações tomadas são válidas ou não. Daí a ideia de aprendizado por reforço. O agente aprende tanto com os erros quanto com os acertos, e no final é esperado que ele execute a tarefa com maestria.\n",
        "\n",
        "Suponha agora que você nunca tenha jogado Pong, o famoso jogo de Atari. Você irá jogá-lo pela primeira vez, mas ninguém te passou as regras e as únicas informações sabidas sobre o funcionamento do jogo é que é possível mover a barra direita para cima ou para baixo e que a tela “Game Over” é o pior resultado possível. O jogo se inicia e, reparando na bola vindo em sua direção, você decide tomar a seguinte abordagem: desviar toda vez que a bola chegar perto de você. O jogo segue, até que o placar atinge 11:0 e a tela “Game Over” é exibida. Para próxima partida, com base em sua experiência anterior, você resolve mover a barra na direção da bola toda vez que ela se aproxima. Com essa nova estratégia você conseguiu vencer o jogo e evitar a tela de “Game Over”.\n",
        "\n",
        "<center>\n",
        "<img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*TcOWlA2MtFazxB91jU4M5Q.gif width=\"500\">\n",
        "</center>\n",
        "\n",
        "Esse exemplo, apesar de um pouco absurdo na perspectiva de um modelo de Reinforcement Learning, pois a evolução do agente geralmente requer muito mais do que apenas um episódio (nesse caso, partida jogada) para aprender a tarefa desejada, ilustra como funciona a lógica do aprendizado por reforço.\n",
        "\n",
        "##Aplications\n",
        "RL agents can solve problems without predefined solutions or explicitly programmed actions and most importantly, without large amounts of data. That’s why RL is having a significant impact on many fields. For instance, it’s used in:\n",
        "\n",
        "- **Self-driving cars**: RL agents can learn optimal driving strategies based on traffic conditions and road rules.\n",
        "- **Robotics**: Robots can be trained to perform complex tasks in dynamic environments through RL.\n",
        "- **Game playing**: AI agents can learn complex strategies in games like Go or StarCraft II using RL techniques.\n",
        "\n",
        "Reinforcement learning is a rapidly evolving field with vast potential. As research progresses, we can expect even more groundbreaking applications in areas like resource management, healthcare, and personalized learning.\n",
        "\n",
        "That’s why now is a great time to learn about this fascinating field of machine learning. In this tutorial, we’ll help you understand the fundamentals of reinforcement learning and explain step-by-step concepts like agent, environment, action, state, rewards, and more."
      ],
      "metadata": {
        "id": "P69R32zzzcba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agente e ambiente\n",
        "Digamos que você queira ensinar seu gato, Bob, a usar vários arranhadores em uma sala em vez de seus móveis caros. Em termos de aprendizado por reforço, Bob é o **agente**, o aprendiz e o tomador de decisões. Ele precisa aprender quais coisas são aceitáveis ​​para arranhar (tapetes e postes) e quais não são (sofás e cortinas).\n",
        "\n",
        "A sala é chamada de **ambiente** com o qual nosso agente interage. Ela fornece desafios (móveis tentadores) e o objetivo desejado (um arranhador satisfatório).\n",
        "\n",
        "Existem dois tipos principais de ambientes na RL:\n",
        "\n",
        "- **Ambientes discretos**: se a sala se transformasse em um jogo clássico de fliperama com uma grade, Bob só poderia se mover em quatro direções. Esses ambientes têm um número limitado de opções para as ações de Bob e as variações da sala.\n",
        "- **Ambientes contínuos**: uma sala do mundo real permite possibilidades quase infinitas para organizar os arranhadores e móveis. Além disso, Bob é livre para fazer qualquer coisa que os gatos costumam fazer todos os dias. É por isso que nossa sala é um ambiente contínuo com infinitas possibilidades para Bob e a sala.\n",
        "\n",
        "Nossa sala também é um ambiente estático. Os móveis não se movem, e os arranhadores permanecem no lugar.\n",
        "\n",
        "Mas se você mover aleatoriamente os móveis e arranhadores uma vez a cada poucas horas (como diferentes níveis do jogo Super Mario), a sala se tornaria um ambiente dinâmico, o que é mais complicado para um agente aprender porque as coisas continuam mudando."
      ],
      "metadata": {
        "id": "aebDWaupaFsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ações e Estados\n",
        "Dois aspectos importantes de todos os problemas de aprendizado por reforço são o **espaço de estados** e o **espaço de ação**.\n",
        "\n",
        "O espaço de estados representa todos os estados possíveis (situações) em que o agente e o ambiente se encontram em qualquer momento. O tamanho do espaço de estados depende do tipo de ambiente:\n",
        "\n",
        "- **Espaço de estados finito**: quando o ambiente é discreto, o número de estados possíveis é finito ou infinito contável. Exemplos são jogos de tabuleiro, videogames em formato de grade ou jogo da velha.\n",
        "- **Espaço de estados infinito**: quando o ambiente é contínuo, seu espaço de estados é ilimitado. Exemplos são videogames com personagens e ambientes complexos, robôs aprendendo a andar ou, no nosso caso, gatos aprendendo a usar um poste de arranhar.\n",
        "\n",
        "O espaço de ação é tudo o que Bob pode fazer no ambiente. No nosso exemplo do poste de arranhar, as ações de Bob podem ser arranhar o poste, tirar uma soneca no sofá ou até mesmo perseguir seu rabo."
      ],
      "metadata": {
        "id": "PFr2FGwYbdfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recompensas, Time Steps e Episódios\n",
        "Para Bob atingir seu objetivo geral, ele precisa de incentivos ou recompensas.\n",
        "\n",
        "A maioria dos problemas de RL tem recompensas predefinidas. Por exemplo, no xadrez, capturar uma peça é uma recompensa positiva, enquanto receber um cheque é uma recompensa negativa.\n",
        "\n",
        "No nosso caso, podemos dar guloseimas a Bob se observarmos uma ação positiva, como não arranhar móveis por algum tempo ou se ele realmente encontrar um dos postes de arranhar. Também podemos puni-lo com alguns jatos de água no rosto se ele arranhar cortinas.\n",
        "\n",
        "Para medir o progresso da jornada de aprendizado de Bob, podemos pensar em suas ações em termos de **time steps**. Por exemplo, na etapa de tempo t1, Bob realiza a ação a1, que resulta em um novo estado s1 (s0 era o estado padrão). Ele também pode receber uma recompensa r1.\n",
        "\n",
        "Uma coleção de time steps é chamada de **episódio**. Um episódio sempre começa em um estado padrão (a mobília e os postes estão montados) e termina quando o objetivo é alcançado (um poste é encontrado) ou o agente falha (arranha a mobília). Às vezes, um episódio também pode terminar com base em quanto tempo passou (como no xadrez)."
      ],
      "metadata": {
        "id": "MFbAe0d9b-aN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exploration vs. Exploitation\n",
        "Like a skilled chess player, Bob shouldn’t just seek any scratching post. Bob should want the one that yields the most rewarding treats. This highlights a classical dilemma in reinforcement learning: **exploration vs. exploitation**.\n",
        "\n",
        "While a tempting post might offer immediate gratification, a more strategic exploration could lead to a jackpot treat later. Just as a chess player might forgo a capture to gain a superior position, Bob might initially scratch a suboptimal post (**exploration**) to discover the ultimate scratching haven (**exploitation**). This long-term strategy is crucial for agents to maximize rewards in complex environments.\n",
        "\n",
        "In other words, Bob must balance exploitation (sticking to what works best) with exploration (occasionally venturing out to look for new scratching posts). Exploring too much might waste time, especially in continuous environments, while exploiting too much might make Bob miss out on something even better.\n",
        "\n",
        "Luckily, there are some clever strategies Bob can take:\n",
        "\n",
        "- **Epsilon-greedy learning**: Let’s imagine for a second that Bob has a special “scratchy-meter” that generates random numbers. If the generated number is smaller than some predefined threshold called epsilon, Bob tries a random scratching surface (exploration). But if the number is greater than epsilon, Bob goes for the post that felt best before (exploitation).\n",
        "- **Boltzmann exploration**: If Bob keeps scratching things that don't feel right (getting negative rewards), it's more likely to explore new options (increased exploration). But as he finds the perfect scratching post (positive rewards), he'll stick to that happy spot (exploitation).\n",
        "\n",
        "By using these strategies (or others that fall beyond the scope of our tutorial), Bob can find a balance between exploring the unknown and sticking to the good stuff."
      ],
      "metadata": {
        "id": "Uo09vetWc-ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aprendizado por reforço\n",
        "O processo de aprendizagem dos algoritmos de aprendizado por reforço (RL) é semelhante ao aprendizado por reforço animal e humano no campo da psicologia comportamental. Por exemplo, uma criança pode descobrir que recebe elogios dos pais quando ajuda um irmão ou faxineira, mas recebe reações negativas quando joga brinquedos ou grita. Logo, a criança aprende qual combinação de atividades resulta na recompensa final.\n",
        "\n",
        "Um algoritmo de RL imita um processo de aprendizagem similar. Experimenta atividades diferentes para aprender os valores negativos e positivos associados a fim de alcançar o resultado final da recompensa.\n",
        "\n",
        "###Principais conceitos\n",
        "No aprendizado por reforço, existem alguns conceitos-chave com os quais você deve se familiarizar:\n",
        "\n",
        "* **Agente** é o algoritmo de ML (ou o sistema autônomo). Entidade, podendo ser tanto um software quanto a sua combinação com hardware, que tomará decisões no ambiente, interagindo com ele tomando determinadas ações e recebendo as recompensas correspondentes. Essa é a entidade que aprende no Reinforcement Learning.\n",
        "- **Ambiente** é o espaço adaptativo do problema com atributos como variáveis, valores limite, regras e ações válidas. É a materialização (ou simulação) do problema a ser resolvido. Podendo ser real ou virtual, este será o espaço no qual o agente realizará suas ações.\n",
        "- **Ação** é uma etapa que o agente de RL executa para navegar pelo ambiente.\n",
        "- **Estado (s)** é o ambiente em um determinado momento. É como o sistema, agente e ambiente, se encontra em um determinado instante. Sempre que o agente realiza uma ação, o ambiente fornece um novo estado e uma recompensa correspondente.\n",
        "- **Política (π)** e a estratégia aplicada pelo agente para decidir a próxima ação com base no estado atual. A política é sempre atualizada para se atingir um comportamento ideal.\n",
        "- **Recompensa (r)** é o valor positivo, negativo ou zero, em outras palavras, a recompensa ou punição, por realizar uma ação. Norteia as ações do agente, sendo a política ideal determinada a partir dela. Se a ação tomada pelo agente em um determinado estado for desejada, a recompensa é positiva, caso contrário, ela é negativa ou nula.\n",
        "- **Recompensa cumulativa** é a soma de todas as recompensas ou o valor final."
      ],
      "metadata": {
        "id": "sB_yOBdfvZ7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reinforcement Learning Algorithms\n",
        "Bob não consegue descobrir como maximizar o número de guloseimas sozinho. Ele precisa de alguns métodos e ferramentas para orientar suas decisões em todos os estados do ambiente. É aqui que os algoritmos de aprendizado por reforço vêm ao resgate de Bob.\n",
        "\n",
        "De uma perspectiva mais ampla, os algoritmos de aprendizado por reforço podem ser categorizados com base em como eles fazem os agentes interagirem com o ambiente e aprenderem com a experiência. As duas principais categorias de algoritmos de aprendizado por reforço são baseadas em modelos e sem modelos.\n",
        "\n",
        "###Model-based reinforcement learning\n",
        "Em algoritmos baseados em modelo, o agente (como Bob) constrói um modelo interno do ambiente. Este modelo representa a dinâmica do ambiente, incluindo transições de estado e probabilidades de recompensa. O agente pode então usar este modelo para planejar e avaliar diferentes ações antes de tomá-las no ambiente real.\n",
        "\n",
        "Esta abordagem tem a vantagem de ser mais eficiente em termos de amostra, especialmente em ambientes complexos. Isto significa que Bob pode exigir menos tentativas de arranhar para identificar o post ideal em comparação com abordagens puramente de tentativa e erro. E isto porque Bob pode planejar e avaliar antes de agir.\n",
        "\n",
        "A desvantagem é que construir um modelo preciso pode ser desafiador, especialmente para ambientes complexos. O modelo pode não refletir com precisão o ambiente real, levando a um comportamento abaixo do ideal.\n",
        "\n",
        "Um algoritmo RL baseado em modelo comum é o **Dyna-Q**, que na verdade combina aprendizagem baseada em modelo e sem modelo. Ele constrói um modelo do ambiente e o utiliza para planejamento de ação enquanto simultaneamente aprende diretamente da experiência por meio do `Q-learning` sem modelo, que será aborado mais a frente.\n",
        "\n",
        "###Model-free (value-based) reinforcement learning\n",
        "Esta abordagem foca em aprender diretamente da interação com o ambiente sem construir explicitamente um modelo interno. O agente (Bob) aprende o valor de estados e ações ou a estratégia ótima por meio de tentativa e erro.\n",
        "\n",
        "A RL sem modelo oferece uma abordagem mais simples em ambientes onde construir um modelo preciso é desafiador. Para Bob, isso significa que ele não precisa criar um mapa mental complexo da sala – ele pode aprender arranhando e vivenciando as consequências.\n",
        "\n",
        "A RL sem modelo se destaca em ambientes dinâmicos onde as regras podem mudar. Se o layout dos móveis da sala mudar, Bob pode adaptar sua exploração e aprender os novos pontos ideais para arranhar.\n",
        "\n",
        "No entanto, aprender apenas por tentativa e erro pode ser menos eficiente em termos de amostra. Bob pode precisar arranhar muitos móveis antes de encontrar consistentemente o poste mais gratificante.\n",
        "\n",
        "Alguns algoritmos comuns de RL sem modelo incluem:\n",
        "- **Q-Learning**: O algoritmo aprende um valor Q para cada par estado-ação. O valor Q representa a recompensa futura esperada de tomar uma ação específica em um estado particular. O agente pode então escolher a ação com o maior valor Q para maximizar sua recompensa de longo prazo.\n",
        "- **SARSA** (Estado-Ação-Recompensa-Estado-Ação): Isso é semelhante ao Q-learning, mas aprende uma função de valor para cada par estado-ação. Ele atualiza o valor com base na recompensa recebida após tomar uma ação e no próximo estado observado.\n",
        "- **Policy gradient methods**: Esses algoritmos aprendem diretamente a função de política, que mapeia estados para ações. Eles usam gradientes para atualizar a política na direção esperada para levar a recompensas maiores. Exemplos incluem REINFORCE e Proximal Policy Optimization (PPO).\n",
        "- **Deep Q-Networks** (DQN): Este algoritmo combina Q-learning com redes neurais profundas para lidar com espaços de estado de alta dimensão, frequentemente encontrados em ambientes complexos como videogames.\n",
        "\n",
        "Deep reinforcement learning algorithm families:\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=14jDgs-JVYflcbxHNYPNTlfgSyDIVCNdG' width=700>\n",
        "</center>\n",
        "\n",
        "O algoritmo que devemos escolher depende de vários fatores: a complexidade do ambiente, a disponibilidade de recursos ou o nível desejado de interpretabilidade.\n",
        "\n",
        "Abordagens baseadas em modelos podem ser preferíveis para ambientes mais simples, onde construir um modelo preciso é viável. Por outro lado, abordagens sem modelo são frequentemente mais práticas para cenários complexos do mundo real.\n",
        "\n",
        "Além disso, com o surgimento do aprendizado profundo, Deep Q-Networks (DQN) e outros algoritmos de RL profundos estão se tornando cada vez mais populares para lidar com tarefas complexas com espaços de estado de alta dimensão."
      ],
      "metadata": {
        "id": "ufxY9iE1tU-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Markov Decision Process (MDP)\n",
        "A lógica do aprendizado por reforço é fácil de entender e visualizar pois se assemelha bastante com a forma como nós aprendemos. O desafio, então, é traduzir essa lógica para uma forma que a máquina possa executá-la. Para isso, é utilizado um modelo matemático conhecido como Markov Decision Process.\n",
        "\n",
        "O processo se dá da seguinte maneira: no instante inicial (t), o **agente** seleciona uma ação, muda para um novo **estado**, recebe uma **recompensa** e então o ciclo é reiniciado para o próximo instante (t+1).\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1I2LjHmCLdYbdA4KLkHx6ebkX05QmHZCP' width=600>\n",
        "</center>"
      ],
      "metadata": {
        "id": "50DiFkPm2zC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The RL algorithm\n",
        "As etapas envolvidas em um algoritmo RL típico são as seguintes:\n",
        "1. Primeiro, o agente interage com o ambiente executando uma\n",
        "ação.\n",
        "2. Ao executar uma ação, o agente se move de um estado para\n",
        "outro.\n",
        "3. Então o agente receberá uma recompensa com base na ação que ele\n",
        "executou.\n",
        "4. Com base na recompensa, o agente entenderá se a\n",
        "ação é boa ou ruim.\n",
        "5. Se a ação foi boa, ou seja, se o agente recebeu uma\n",
        "recompensa positiva, então o agente preferirá executar essa ação, caso contrário\n",
        "o agente tentará executar outras ações em busca de uma\n",
        "recompensa positiva.\n",
        "\n",
        "RL é basicamente um processo de aprendizado de tentativa e erro. Como exemplo, no jogo de xadrez, o agente (programa de software) é o jogador\n",
        "de xadrez. Então, o agente interage com o ambiente (tabuleiro de xadrez)\n",
        "executando uma ação (movimentos). Se o agente obtiver uma recompensa positiva por\n",
        "uma ação, então ele preferirá executar essa ação; caso contrário, ele encontrará uma\n",
        "ação diferente que dê uma recompensa positiva."
      ],
      "metadata": {
        "id": "r8GdYselIhx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Referências\n",
        "[1] Dong H. **Deep Reinforcement Learning. Fundamentals, Research and Applications**. Springer. 2020.\n",
        "\n",
        "[2] Laura Graesser, Wah Loon Keng. **Foundations of Deep Reinforcement Learning - Theory and Practice in Python**. Pearson Addison-Wesley. 2023.\n",
        "\n",
        "[3] https://aws.amazon.com/pt/what-is/reinforcement-learning/\n",
        "\n",
        "[4] https://www.deeplearningbook.com.br/o-que-e-aprendizagem-por-reforco/\n",
        "\n",
        "[5] https://www.geeksforgeeks.org/what-is-reinforcement-learning/\n",
        "\n",
        "[6] https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "[7] https://www.datacamp.com/pt/tutorial/reinforcement-learning-python-introduction\n",
        "\n",
        "[8] https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b\n",
        "\n",
        "[9] https://machinelearningmastery.com/principles-of-reinforcement-learning-an-introduction-with-python/\n",
        "\n",
        "[10] https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-1-introdu%C3%A7%C3%A3o-7382ebb641ab\n",
        "\n",
        "[11] https://www.datacamp.com/tutorial/reinforcement-learning-python-introduction\n",
        "\n",
        "[12] https://gymnasium.farama.org/introduction/basic_usage/\n",
        "\n",
        "[13] Sudharsan Ravichandiran. **Deep Reinforcement Learning with Python**. Second Edition. Packt Publishing. 2020.\n",
        "\n",
        "[14] https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
      ],
      "metadata": {
        "id": "1mPVxR3d8K68"
      }
    }
  ]
}